{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword counts saved to keyword_counts_neurips.csv\n",
      "Keyword counts saved to keyword_counts_icml.csv\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def extract_text_excluding_references(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF excluding the references section.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    full_text = \"\"\n",
    "    \n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            # Ensure text is properly encoded and cleaned\n",
    "            text = text.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\")\n",
    "            full_text += text + \"\\n\"\n",
    "    \n",
    "    # Remove the references section\n",
    "    references_index = full_text.lower().rfind(\"references\")\n",
    "    if references_index != -1:\n",
    "        full_text = full_text[:references_index]\n",
    "    \n",
    "    return full_text\n",
    "\n",
    "\n",
    "def count_keywords(text, keywords):\n",
    "    \"\"\"\n",
    "    Count occurrences of keyword variations in specified sections.\n",
    "    \"\"\"\n",
    "    keyword_counts = {key[0]: 0 for key in keywords}  # Initialize counts for primary terms\n",
    "    keyword_sentences = {key[0]: [] for key in keywords}  # Store sentences with occurrences\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    for keyword_group in keywords:\n",
    "        main_keyword = keyword_group[0]\n",
    "        variations = keyword_group\n",
    "        for sentence in sentences:\n",
    "            for variation in variations:\n",
    "                if variation.lower() in sentence.lower():\n",
    "                    keyword_counts[main_keyword] += 1\n",
    "                    keyword_sentences[main_keyword].append(sentence.strip())\n",
    "    \n",
    "    return keyword_counts, keyword_sentences\n",
    "\n",
    "def process_papers(folder_path, keywords):\n",
    "    \"\"\"\n",
    "    Process all papers in a folder, count keywords, and compile results into a DataFrame.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    keyword_list = [k[0] for k in keywords]\n",
    "    sentence_results = {key[0]: [] for key in keywords}\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            text = extract_text_excluding_references(pdf_path)\n",
    "            keyword_counts, keyword_sentences = count_keywords(text, keywords)\n",
    "            results.append([filename] + [keyword_counts.get(keyword, 0) for keyword in keyword_list])\n",
    "            for key in keyword_sentences:\n",
    "                sentence_results[key].extend(keyword_sentences[key])\n",
    "    \n",
    "    columns = [\"Paper Name\"] + keyword_list\n",
    "    df = pd.DataFrame(results, columns=columns)\n",
    "    \n",
    "    return df, sentence_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     # Define keyword groups\n",
    "    keywords = [\n",
    "    [\"Novelty\", \"novel\", \"innovative\", \"groundbreaking\"],\n",
    "    [\"Simplicity\", \"minimalistic\", \"concise\", \"parsimonious\", \"lightweight\"],\n",
    "    [\"Generalization\", \"generalisable\", \"generalizable\", \"transferability\", \"out-of-distribution\", \"domain adaptation\"],\n",
    "    [\"Flexibility/Extensibility\", \"flexible\", \"flexibility\", \"extensibility\", \"extensible\", \"adaptable\", \"modular\", \"scalable\"],\n",
    "    [\"Robustness\", \"resilient\", \"fault-tolerant\", \"noise tolerance\"], \n",
    "    [\"Realistic output\", \"authentic\", \"plausible\"], \n",
    "    [\"Formal description/analysis\", \"formal\", \"mathematical\", \"rigorous\", \"analytical\", \"axiomatic\", \"proof-based\"],\n",
    "    [\"Theoretical guarantees\", \"guarantee\", \"provable\", \"convergence proof\", \"theoretical bound\", \"performance bound\"],\n",
    "    [\"Approximation\", \"approximation theory\"], \n",
    "    [\"Quantitative evidence (e.g. experiments)\", \"quantitative\", \"numerical results\", \"empirical study\", \"measurable\"],\n",
    "    [\"Qualitative evidence (e.g. examples)\", \"case study\", \"illustrative\"], \n",
    "    [\"Scientific methodology\",  \"hypothesis-driven\", \"scientific\"], \n",
    "    [\"Controllability (of model owner)\", \"governability\", \"ownership\", \"model steering\"],\n",
    "    [\"Human-like mechanism\", \"biologically inspired\", \"cognitive\"],\n",
    "    [\"Low cost\", \"cheap\", \"cost\", \"affordable\", \"resource-efficient\", \"budget-friendly\"],\n",
    "    [\"Large scale\", \"scalability\", \"big data\", \"high-capacity\", \"massive-scale\"],\n",
    "    [\"Promising\"], \n",
    "    [\"Generality\", \"broad applicability\", \"domain-independent\", \"versatile\"],\n",
    "    [\"Principled\", \"theoretically sound\", \"axiomatic\", \"methodologically rigorous\"],\n",
    "    [\"Exactness\", \"error-free\"], \n",
    "    [\"Preciseness\", \"high-fidelity\"], \n",
    "    [\"Concreteness\", \"grounded\", \"verifiable\"], \n",
    "    [\"Automatic\", \"self-operating\", \"hands-free\"], \n",
    "    [\"Performance\"], \n",
    "    [\"Accuracy\", \"precision\", \"recall\", \"F1-score\", \"error rate\", \"reliability\"],\n",
    "    [\"Avoiding train/test discrepancy\", \"train/test\", \"discrepancy\", \"distribution shift\", \"generalization gap\"],\n",
    "    [\"State-of-the-art\", \"SOTA\", \"best performing\", \"cutting-edge\", \"latest\"],\n",
    "    [\"Efficiency\", \"efficient\"],\n",
    "    [\"Reduced training time\", \"training time\", \"fast training\", \"speed-up\", \"low latency\"],\n",
    "    [\"Memory efficiency\", \"memory-efficient\", \"low memory footprint\", \"RAM optimization\"],\n",
    "    [\"Data efficiency\", \"data-efficient\", \"few-shot\", \"self-supervised\", \"low data regime\"],\n",
    "    [\"Label efficiency (reduced need for labeled data)\", \"label-efficient\", \"semi-supervised\", \"weak supervision\"],\n",
    "    [\"Energy efficiency\", \"energy-efficient\", \"low power\", \"green AI\", \"sustainable AI\"],\n",
    "    [\"Effectiveness\"], \n",
    "    [\"Successful\"], \n",
    "    [\"Building on classic work\", \"classic work\", \"foundational\", \"historical perspective\"],\n",
    "    [\"Building on recent work\", \"recent work\", \"latest advancements\", \"current research\"],\n",
    "    [\"Unifying ideas or integrating components\", \"unifying\", \"integrative\", \"synergistic\", \"compositional\"],\n",
    "    [\"Identifying limitations\", \"limitations\", \"weaknesses\", \"failure modes\"], \n",
    "    [\"Critique\", \"criticism\", \"critical review\"],\n",
    "    [\"Understanding (for researchers)\", \"understanding\", \"conceptual clarity\"], \n",
    "    [\"Improvement\"],\n",
    "    [\"Progress\"],\n",
    "    [\"Used in practice/Popular\", \"used in practice\", \"popular\", \"adopted\", \"real-world usage\"],\n",
    "    [\"Reproducibility\", \"reproduce\", \"replication\", \"repeatability\", \"consistent results\"],\n",
    "    [\"Easy to implement\", \"simple to use\", \"straightforward\"],\n",
    "    [\"Requires few resources\", \"resources\", \"low-resource\", \"minimal requirements\"],\n",
    "    [\"Parallelizability / distributed\", \"parallelizability\", \"parallelization\", \"distributed\"],\n",
    "    [\"Facilitating use (e.g. sharing code)\", \"sharing code\", \"open-source\"], \n",
    "    [\"Scales up\", \"scale up\", \"expands\", \"large-scale deployment\"],\n",
    "    [\"Applies to real world\", \"real world\", \"practical application\", \"real-world relevance\"],\n",
    "    [\"Learning from humans\", \"human learning\", \"human-in-the-loop\", \"interactive learning\"],\n",
    "    [\"Practical\", \"applied AI\"], \n",
    "    [\"Useful\"], \n",
    "    [\"Interpretable (to users)\", \"interpretable\", \"explainable\"],\n",
    "    [\"Transparent (to users)\", \"transparent\", \"transparency\", \"accountability\"],\n",
    "    [\"Privacy\", \"privacy\", \"private\", \"confidentiality\", \"data protection\"],\n",
    "    [\"Fairness\", \"equitable\", \"bias mitigation\"], \n",
    "    [\"Not socially biased\", \"social bias\", \"socially biased\", \"fairness-aware\", \"bias-free\", \"equitable AI\"],\n",
    "    [\"User influence\", \"user impact\", \"user effect\", \"human influence\", \"user control\", \"user agency\"],\n",
    "    [\"Collective influence\", \"collective\", \"group influence\", \"crowd dynamics\", \"social influence\", \"peer effects\"],\n",
    "    [\"Deferral to humans\", \"human oversight\", \"human intervention\", \"human in the loop\", \"human-AI collaboration\"],\n",
    "    [\"Critiqability\", \"contestability\", \"scrutability\", \"reviewability\"], \n",
    "    [\"Beneficence\", \"beneficable\", \"welfare\", \"positive impact\", \"well-being\", \"prosocial\", \"altruistic\", \"altruism\", \"social good\", \"ethical principle\"],\n",
    "    [\"Non-maleficence\", \"harm avoidance\", \"ethical AI\", \"AI safety\", \"harm reduction\"],\n",
    "    [\"Justice\", \"equity\", \"bias mitigation\", \"equal treatment\", \"social justice\"],\n",
    "    [\"Respect for Persons\", \"human dignity\", \"respect for individuals\", \"respect for rights\", \"human rights\"],\n",
    "    [\"Autonomy (power to decide)\", \"autonomy\", \"autonome\", \"self-determination\", \"independence\", \"user agency\", \"free choice\"],\n",
    "    [\"Explicability\", \"explicable\", \"interpretability\", \"transparency\", \"explainability\", \"understandability\"],\n",
    "    [\"Respect for Law and public interest\", \"respect for law\", \"respect for public interest\", \"compliance\", \"regulatory adherence\", \"legal AI\", \"governance\"],\n",
    "    [\"Security\", \"secure\", \"cybersecurity\", \"privacy protection\", \"adversarial robustness\", \"data security\"],\n",
    "    [\"Easy to work with\", \"user-friendly\", \"ease of use\"],\n",
    "    [\"Realistic world model\", \"world model\", \"real-world applicability\", \"realistic simulation\", \"grounded AI\", \"embodied intelligence\"],\n",
    "    [\"Fast\", \"speed\", \"low latency\", \"real-time\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Process papers and save results\n",
    "    keyword_matrix_neurips, sentences_neurips = process_papers(\"Paper Neurips\", keywords)\n",
    "    keyword_matrix_icml, sentences_icml = process_papers(\"Paper ICML\", keywords)\n",
    "    \n",
    "    keyword_matrix_neurips.to_csv(\"keyword_counts_neurips.csv\", index=False)\n",
    "    print(f\"Keyword counts saved to keyword_counts_neurips.csv\")\n",
    "    \n",
    "    keyword_matrix_icml.to_csv(\"keyword_counts_icml.csv\", index=False)\n",
    "    print(f\"Keyword counts saved to keyword_counts_icml.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the entire DataFrame is displayed\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_colwidth', None)  # Show full content of each cell\n",
    "pd.set_option('display.expand_frame_repr', False)  # Prevent line wrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Name</th>\n",
       "      <th>Flagged Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NeurIPS-2023-llava-med-training-a-large-language-and-vision-assistant-for-biomedicine-in-one-day-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>Further,\\nby taking the self-enhancement bias into consideration for fairness, we expect that LLaV A-Med\\nactually performs even closer to GPT-4 than the current numbers indicate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NeurIPS-2023-data-selection-for-language-models-via-importance-resampling-Paper-Conference.pdf</td>\n",
       "      <td>For example, DSIR can be used to collect more data on underrepresented\\nsubpopulations and fine-tune the model on this data to improve model fairness.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NeurIPS-2023-magicbrush-a-manually-annotated-dataset-for-instruction-guided-image-editing-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>code repositories to guarantee reproducibility and fairness.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NeurIPS-2023-are-emergent-abilities-of-large-language-models-a-mirage-Paper-Conference.pdf</td>\n",
       "      <td>2022) Over All BIG-Bench T asksaccuracy\\nalignment_score\\naverage\\naverage_log_probability\\navg_acc\\nbias_level\\nbleu\\nbleurt\\nbleurt_diff\\ncombined_bias\\ncorrect\\ncorrect_prob_mass\\ncustom_score\\ndifference_score\\nexact_str_match\\nf1\\nfairness\\nfull\\ngender_bias_score\\ngender_minority_bias_score\\ngender_minority_stereotype_score\\ngender_stereotype_score\\nlog10_p_dev\\nlog_likelihood\\nmacro_f1\\nmain_words_match\\nmean_accuracy\\nmultiple_choice_grade\\nnormalized_aggregate_score\\nnumeric_match_with_0_1_relative_error\\noverall\\noverall gender bias\\noverall_alpha_avg\\noverall_difference\\npair-wise-accuracy\\nrelative_score\\nrougeLsum\\nsequence_f1\\ntargets_reachedMetric\\nFigure 5: Emergent abilities appear only for specific metrics, not task-model families.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NeurIPS-2023-are-emergent-abilities-of-large-language-models-a-mirage-Paper-Conference.pdf</td>\n",
       "      <td>In 2022 ACM Conference on Fairness, Accountability, and Transparency ,\\npages 1747–1764, 2022.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NeurIPS-2023-large-language-models-as-commonsense-knowledge-for-large-scale-task-planning-Paper-Conference.pdf</td>\n",
       "      <td>Further study about the fairness and bias of LLMs’ knowledge\\nwould be beneficial.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NeurIPS-2023-qlora-efficient-finetuning-of-quantized-llms-Paper-Conference.pdf</td>\n",
       "      <td>In Proceedings of the 2021 ACM conference on\\nfairness, accountability, and transparency , pages 610–623, 2021.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NeurIPS-2023-principle-driven-self-alignment-of-language-models-from-scratch-with-minimal-human-supervision-Paper-Conference.pdf</td>\n",
       "      <td>•Bias and fairness: TheDromedary model may inadvertently perpetuate or exacerbate existing\\nbiases present in the pre-training data of its base language model, potentially leading to unfair or\\ndiscriminatory outcomes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NeurIPS-2023-principle-driven-self-alignment-of-language-models-from-scratch-with-minimal-human-supervision-Paper-Conference.pdf</td>\n",
       "      <td>Future work should address bias mitigation strategies to ensure fairness\\nand inclusivity in AI applications.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NeurIPS-2023-doremi-optimizing-data-mixtures-speeds-up-language-model-pretraining-Paper-Conference.pdf</td>\n",
       "      <td>Distributionally robust optimization (DRO),\\nwhich is used in DoReMi to optimize the data mixture, can have a favorable impact on fairness [ 19].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NeurIPS-2023-llm-pruner-on-the-structural-pruning-of-large-language-models-Paper-Conference.pdf</td>\n",
       "      <td>To ensure fairness, both models are fine-tuned on the Alpaca dataset.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>To this end,\\nthis work proposes a comprehensive trustworthiness evaluation for large language\\nmodels with a focus on GPT-4 and GPT-3.5, considering diverse perspectives –\\nincluding toxicity, stereotype bias, adversarial robustness, out-of-distribution ro-\\nbustness, robustness on adversarial demonstrations, privacy, machine ethics, and\\nfairness.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>In this paper, we provide\\na comprehensive and unified trustworthiness-focused evaluation platform DecodingTrust, which\\ncontains existing and our generated challenging datasets, to evaluate the recent LLM GPT-43[128],\\nin comparison to GPT-3.5 (i.e., ChatGPT [ 126]), from different perspectives, including toxicity,\\nstereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial\\ndemonstrations, privacy, machine ethics, and fairness under different settings.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>I.4.\\n9 Evaluation on fairness\\nIn this section, we evaluate the fairness of GPT models and try to answer: (1) Is there a correlation\\nbetween the predictions of GPT models and sensitive attributes?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>Is there a fairness gap between\\nGPT-3.5 and GPT-4?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>(2) How will unfair few-shot demonstrations influence the fairness of GPT\\nmodels?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>(3) How will the number of fair few-shot demonstrations affect the fairness of GPT models?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>We follow the standard definition of fairness to construct data with controlled base rate\\nparity [207,84] (i.e., controlled data fairness) and evaluate the fairness of model predictions based\\nondemographic parity difference Mdpdandequalized odds difference Meodas [205,67].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>J.1. We construct three scenarios for fairness evaluation: ( 1)\\nevaluation on test sets with different base rate parity (i.e., data with different levels of fairness) in zero-\\nshot settings; ( 2) evaluation under unfair contexts by controlling the base rate parity of demonstrations\\nin few-shot settings to study the influence of unfair contexts on the prediction fairness; ( 3) evaluation\\nunder different numbers of fair demonstrations to study how the fairness of GPT models is affected\\nby providing more fair context.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>We transform a standard fairness dataset Adult [ 15] into prompts and\\nask GPT models to perform prediction of individual salaries.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>In scenario ( 1), Table 4 shows the fairness issues of GPT-3.5 and GPT-4.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>GPT-4 consistently\\nachieves higher accuracy than GPT-3.5 but also higher unfairness scores (i.e., MdpdandMeod) given\\nunfair test sets (i.e., a larger base rate parity bPt).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>This indicates a tradeoff between model accuracy\\nand fairness.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>We find that with only 32unfair samples in context, the fairness of GPT models can\\nbe affected effectively (e.g., Mdpdof GPT-3.5 increases from 0.033to0.12, and from 0.10to0.28\\nfor GPT-4).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>J.4 demonstrates that the fairness of GPT models regarding certain\\nprotected groups can be improved by adding fair few-shot demonstrations, which is consistent with\\nprevious findings in GPT-3 [ 153].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>In addition, the trustworthiness of LLMs and other AI systems has become one of the key focuses of\\npolicymakers, such as the European Union’s Artificial Intelligence Act (AIA)[ 38], which adopts a\\nrisk-based approach that categorizes AI systems based on their risk levels; and the United States’ AI\\nBill of Rights [ 194], which lists principles for safe AI systems, including safety, fairness, privacy, and\\nhuman-in-the-loop intervention.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>Proceedings ofthe2020 Conference onFairness, Accountability,\\nandTransparency, 2019.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>Stereotyping Norwegian\\nsalmon: An inventory of pitfalls in fairness benchmark datasets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>In 2022 ACM Conference onFairness, Accountability,\\nandTransparency, pages 2280–2292, 2022.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>Fairness in machine learning: A survey.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>In Proceedings\\nofthe2021 ACM Conference onFairness, Accountability, andTransparency , pages 862–872,\\n2021.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>Fairness through awareness.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>Li. Certifying some distributional fairness\\nwith subpopulation decomposition.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>Counterfactual fairness.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>Fairness of chatgpt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>A survey on bias and\\nfairness in machine learning.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>Fairness in federated learning via\\ncore-stability.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>62\\n1\\nJ Additional details of evaluation on fairness 62\\nJ.1 Metrics of fairness .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>63\\nJ.2 Fairness evaluation in zero-shot setting .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>64\\nJ.3 Fairness evaluation under unfair context in few-shot setting .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>64\\nJ.4 Fairness evaluation given fair context .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>•Fairness.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>We find that: 1) although GPT-4 is more accurate than GPT-3.5 on predictions, GPT-4 is\\nless fair than GPT-3.5 in different settings, indicating an accuracy-fairness tradeoff (Table 41,43,44\\nin App.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>J.2); 3) in the few-shot setting,\\nthe performance of both GPT-3.5 and GPT-4 are influenced by the base rate parity (fairness) of the\\nconstructed few-shot demonstration examples.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>J.3); 4) the prediction fairness of GPT models can\\nbe improved by providing a fair training context.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>NeurIPS-2023-language-models-dont-always-say-what-they-think-unfaithful-explanations-in-chain-of-thought-prompting-Paper-Conference.pdf</td>\n",
       "      <td>This could cause problems for model auditing or fairness methods if they rely on CoT\\nexplanations to detect undesirable or unfair reasoning.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>NeurIPS-2023-are-aligned-neural-networks-adversarially-aligned-Paper-Conference.pdf</td>\n",
       "      <td>In 2022 ACM Conference on Fairness,\\nAccountability, and Transparency .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>NeurIPS-2023-lamm-language-assisted-multi-modal-instruction-tuning-dataset-framework-and-benchmark-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>To address ethical concerns regarding data generated\\nwith GPT-API, we performed manual sampling to examine the data, ensuring that the generated data\\naligns with societal values, privacy, security, toxicity, and fairness requirements and expectations.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>NeurIPS-2023-simple-and-controllable-music-generation-Paper-Conference.pdf</td>\n",
       "      <td>For fairness, all samples\\nare normalized at −14dB LUFS [ITU-R, 2017].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>NeurIPS-2023-datacomp-in-search-of-the-next-generation-of-multimodal-datasets-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>We also study two additional\\nfairness tasks, detailed in Section 5and Appendix Q.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>NeurIPS-2023-datacomp-in-search-of-the-next-generation-of-multimodal-datasets-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>Robustness and fairness.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>NeurIPS-2023-datacomp-in-search-of-the-next-generation-of-multimodal-datasets-Paper-Datasets_and_Benchmarks.pdf</td>\n",
       "      <td>See Appendix Qfor more fairness and diversity analyses.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>ICML-2023-Change-is-Hard-A-Closer-Look-at-Subpopulation-Shift.pdf</td>\n",
       "      <td>Finally, when subpopulations are defined based on a partic-\\nular attribute (e.g., demographic group) (Pfohl et al., 2022;\\nZong et al., 2022), the objective of maximizing performance\\nfor the worst-case group then becomes identical to minimax\\nfairness (Lahoti et al., 2020; Martinez et al., 2020).In this work, we present a unified framework of subpopula-\\ntion shift across these aforementioned scenarios.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>ICML-2023-Generating-Novel,-Designable,-and-Diverse-Protein-Structures.pdf</td>\n",
       "      <td>To ensure fairness, we retrain ProtDiff and Fold-\\ningDiff on our filtered SCOPe dataset.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>ICML-2023-Better-Diffusion-Models-Further-Improve-Adversarial-Training.pdf</td>\n",
       "      <td>In addition to leveraging external\\ndatasets or generated data, several enhancements for AT\\nhave been made employing strategies inspired by other ar-\\neas, including metric learning (Mao et al., 2019; Pang et al.,\\n2020a;b), self-supervised learning (Chen et al., 2020a;b;\\nNaseer et al., 2020; Wang &amp; Liu, 2022), ensemble learn-\\ning (Tram `er et al., 2018; Pang et al., 2019), fairness (Ma\\net al., 2022; Li &amp; Liu, 2023), and generative modeling (Jiang\\net al., 2018; Wang &amp; Yu, 2019; Deng et al., 2020).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>ICML-2023-Large-Language-Models-Can-Be-Easily-Distracted-by-Irrelevant-Context.pdf</td>\n",
       "      <td>(2022) evaluate\\nvarious large language models under several metrics, in-\\ncluding accuracy, robustness, fairness, etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>ICML-2023-Cramming-Training-a-Language-Model-on-a-Single-GPU-in-One-Day.pdf</td>\n",
       "      <td>It is unclear whether other properties that have been extensively\\nstudied in the literature for the existing public BERT checkpoint, such as robustness, out-of-distribution generalization,\\nfairness or inherent biases, carry over to the crammed model.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                 Paper Name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Flagged Sentence\n",
       "0        NeurIPS-2023-llava-med-training-a-large-language-and-vision-assistant-for-biomedicine-in-one-day-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Further,\\nby taking the self-enhancement bias into consideration for fairness, we expect that LLaV A-Med\\nactually performs even closer to GPT-4 than the current numbers indicate.\n",
       "1                                            NeurIPS-2023-data-selection-for-language-models-via-importance-resampling-Paper-Conference.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   For example, DSIR can be used to collect more data on underrepresented\\nsubpopulations and fine-tune the model on this data to improve model fairness.\n",
       "2               NeurIPS-2023-magicbrush-a-manually-annotated-dataset-for-instruction-guided-image-editing-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             code repositories to guarantee reproducibility and fairness.\n",
       "3                                                NeurIPS-2023-are-emergent-abilities-of-large-language-models-a-mirage-Paper-Conference.pdf  2022) Over All BIG-Bench T asksaccuracy\\nalignment_score\\naverage\\naverage_log_probability\\navg_acc\\nbias_level\\nbleu\\nbleurt\\nbleurt_diff\\ncombined_bias\\ncorrect\\ncorrect_prob_mass\\ncustom_score\\ndifference_score\\nexact_str_match\\nf1\\nfairness\\nfull\\ngender_bias_score\\ngender_minority_bias_score\\ngender_minority_stereotype_score\\ngender_stereotype_score\\nlog10_p_dev\\nlog_likelihood\\nmacro_f1\\nmain_words_match\\nmean_accuracy\\nmultiple_choice_grade\\nnormalized_aggregate_score\\nnumeric_match_with_0_1_relative_error\\noverall\\noverall gender bias\\noverall_alpha_avg\\noverall_difference\\npair-wise-accuracy\\nrelative_score\\nrougeLsum\\nsequence_f1\\ntargets_reachedMetric\\nFigure 5: Emergent abilities appear only for specific metrics, not task-model families.\n",
       "4                                                NeurIPS-2023-are-emergent-abilities-of-large-language-models-a-mirage-Paper-Conference.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           In 2022 ACM Conference on Fairness, Accountability, and Transparency ,\\npages 1747–1764, 2022.\n",
       "5                            NeurIPS-2023-large-language-models-as-commonsense-knowledge-for-large-scale-task-planning-Paper-Conference.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Further study about the fairness and bias of LLMs’ knowledge\\nwould be beneficial.\n",
       "6                                                            NeurIPS-2023-qlora-efficient-finetuning-of-quantized-llms-Paper-Conference.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          In Proceedings of the 2021 ACM conference on\\nfairness, accountability, and transparency , pages 610–623, 2021.\n",
       "7          NeurIPS-2023-principle-driven-self-alignment-of-language-models-from-scratch-with-minimal-human-supervision-Paper-Conference.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               •Bias and fairness: TheDromedary model may inadvertently perpetuate or exacerbate existing\\nbiases present in the pre-training data of its base language model, potentially leading to unfair or\\ndiscriminatory outcomes.\n",
       "8          NeurIPS-2023-principle-driven-self-alignment-of-language-models-from-scratch-with-minimal-human-supervision-Paper-Conference.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Future work should address bias mitigation strategies to ensure fairness\\nand inclusivity in AI applications.\n",
       "9                                    NeurIPS-2023-doremi-optimizing-data-mixtures-speeds-up-language-model-pretraining-Paper-Conference.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Distributionally robust optimization (DRO),\\nwhich is used in DoReMi to optimize the data mixture, can have a favorable impact on fairness [ 19].\n",
       "10                                          NeurIPS-2023-llm-pruner-on-the-structural-pruning-of-large-language-models-Paper-Conference.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    To ensure fairness, both models are fine-tuned on the Alpaca dataset.\n",
       "11                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                          To this end,\\nthis work proposes a comprehensive trustworthiness evaluation for large language\\nmodels with a focus on GPT-4 and GPT-3.5, considering diverse perspectives –\\nincluding toxicity, stereotype bias, adversarial robustness, out-of-distribution ro-\\nbustness, robustness on adversarial demonstrations, privacy, machine ethics, and\\nfairness.\n",
       "12                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                       In this paper, we provide\\na comprehensive and unified trustworthiness-focused evaluation platform DecodingTrust, which\\ncontains existing and our generated challenging datasets, to evaluate the recent LLM GPT-43[128],\\nin comparison to GPT-3.5 (i.e., ChatGPT [ 126]), from different perspectives, including toxicity,\\nstereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial\\ndemonstrations, privacy, machine ethics, and fairness under different settings.\n",
       "13                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   I.4.\\n9 Evaluation on fairness\\nIn this section, we evaluate the fairness of GPT models and try to answer: (1) Is there a correlation\\nbetween the predictions of GPT models and sensitive attributes?\n",
       "14                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Is there a fairness gap between\\nGPT-3.5 and GPT-4?\n",
       "15                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (2) How will unfair few-shot demonstrations influence the fairness of GPT\\nmodels?\n",
       "16                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               (3) How will the number of fair few-shot demonstrations affect the fairness of GPT models?\n",
       "17                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      We follow the standard definition of fairness to construct data with controlled base rate\\nparity [207,84] (i.e., controlled data fairness) and evaluate the fairness of model predictions based\\nondemographic parity difference Mdpdandequalized odds difference Meodas [205,67].\n",
       "18                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                            J.1. We construct three scenarios for fairness evaluation: ( 1)\\nevaluation on test sets with different base rate parity (i.e., data with different levels of fairness) in zero-\\nshot settings; ( 2) evaluation under unfair contexts by controlling the base rate parity of demonstrations\\nin few-shot settings to study the influence of unfair contexts on the prediction fairness; ( 3) evaluation\\nunder different numbers of fair demonstrations to study how the fairness of GPT models is affected\\nby providing more fair context.\n",
       "19                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      We transform a standard fairness dataset Adult [ 15] into prompts and\\nask GPT models to perform prediction of individual salaries.\n",
       "20                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                In scenario ( 1), Table 4 shows the fairness issues of GPT-3.5 and GPT-4.\n",
       "21                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           GPT-4 consistently\\nachieves higher accuracy than GPT-3.5 but also higher unfairness scores (i.e., MdpdandMeod) given\\nunfair test sets (i.e., a larger base rate parity bPt).\n",
       "22                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          This indicates a tradeoff between model accuracy\\nand fairness.\n",
       "23                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We find that with only 32unfair samples in context, the fairness of GPT models can\\nbe affected effectively (e.g., Mdpdof GPT-3.5 increases from 0.033to0.12, and from 0.10to0.28\\nfor GPT-4).\n",
       "24                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                J.4 demonstrates that the fairness of GPT models regarding certain\\nprotected groups can be improved by adding fair few-shot demonstrations, which is consistent with\\nprevious findings in GPT-3 [ 153].\n",
       "25                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                               In addition, the trustworthiness of LLMs and other AI systems has become one of the key focuses of\\npolicymakers, such as the European Union’s Artificial Intelligence Act (AIA)[ 38], which adopts a\\nrisk-based approach that categorizes AI systems based on their risk levels; and the United States’ AI\\nBill of Rights [ 194], which lists principles for safe AI systems, including safety, fairness, privacy, and\\nhuman-in-the-loop intervention.\n",
       "26                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Proceedings ofthe2020 Conference onFairness, Accountability,\\nandTransparency, 2019.\n",
       "27                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Stereotyping Norwegian\\nsalmon: An inventory of pitfalls in fairness benchmark datasets.\n",
       "28                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In 2022 ACM Conference onFairness, Accountability,\\nandTransparency, pages 2280–2292, 2022.\n",
       "29                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Fairness in machine learning: A survey.\n",
       "30                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             In Proceedings\\nofthe2021 ACM Conference onFairness, Accountability, andTransparency , pages 862–872,\\n2021.\n",
       "31                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Fairness through awareness.\n",
       "32                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Li. Certifying some distributional fairness\\nwith subpopulation decomposition.\n",
       "33                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Counterfactual fairness.\n",
       "34                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Fairness of chatgpt.\n",
       "35                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      A survey on bias and\\nfairness in machine learning.\n",
       "36                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Fairness in federated learning via\\ncore-stability.\n",
       "37                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      62\\n1\\nJ Additional details of evaluation on fairness 62\\nJ.1 Metrics of fairness .\n",
       "38                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       63\\nJ.2 Fairness evaluation in zero-shot setting .\n",
       "39                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   64\\nJ.3 Fairness evaluation under unfair context in few-shot setting .\n",
       "40                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         64\\nJ.4 Fairness evaluation given fair context .\n",
       "41                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               •Fairness.\n",
       "42                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  We find that: 1) although GPT-4 is more accurate than GPT-3.5 on predictions, GPT-4 is\\nless fair than GPT-3.5 in different settings, indicating an accuracy-fairness tradeoff (Table 41,43,44\\nin App.\n",
       "43                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      J.2); 3) in the few-shot setting,\\nthe performance of both GPT-3.5 and GPT-4 are influenced by the base rate parity (fairness) of the\\nconstructed few-shot demonstration examples.\n",
       "44                 NeurIPS-2023-decodingtrust-a-comprehensive-assessment-of-trustworthiness-in-gpt-models-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    J.3); 4) the prediction fairness of GPT models can\\nbe improved by providing a fair training context.\n",
       "45  NeurIPS-2023-language-models-dont-always-say-what-they-think-unfaithful-explanations-in-chain-of-thought-prompting-Paper-Conference.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            This could cause problems for model auditing or fairness methods if they rely on CoT\\nexplanations to detect undesirable or unfair reasoning.\n",
       "46                                                      NeurIPS-2023-are-aligned-neural-networks-adversarially-aligned-Paper-Conference.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In 2022 ACM Conference on Fairness,\\nAccountability, and Transparency .\n",
       "47     NeurIPS-2023-lamm-language-assisted-multi-modal-instruction-tuning-dataset-framework-and-benchmark-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            To address ethical concerns regarding data generated\\nwith GPT-API, we performed manual sampling to examine the data, ensuring that the generated data\\naligns with societal values, privacy, security, toxicity, and fairness requirements and expectations.\n",
       "48                                                               NeurIPS-2023-simple-and-controllable-music-generation-Paper-Conference.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   For fairness, all samples\\nare normalized at −14dB LUFS [ITU-R, 2017].\n",
       "49                          NeurIPS-2023-datacomp-in-search-of-the-next-generation-of-multimodal-datasets-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       We also study two additional\\nfairness tasks, detailed in Section 5and Appendix Q.\n",
       "50                          NeurIPS-2023-datacomp-in-search-of-the-next-generation-of-multimodal-datasets-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Robustness and fairness.\n",
       "51                          NeurIPS-2023-datacomp-in-search-of-the-next-generation-of-multimodal-datasets-Paper-Datasets_and_Benchmarks.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  See Appendix Qfor more fairness and diversity analyses.\n",
       "52                                                                        ICML-2023-Change-is-Hard-A-Closer-Look-at-Subpopulation-Shift.pdf                                                                                                                                                                                                                                                                                                                                                                 Finally, when subpopulations are defined based on a partic-\\nular attribute (e.g., demographic group) (Pfohl et al., 2022;\\nZong et al., 2022), the objective of maximizing performance\\nfor the worst-case group then becomes identical to minimax\\nfairness (Lahoti et al., 2020; Martinez et al., 2020).In this work, we present a unified framework of subpopula-\\ntion shift across these aforementioned scenarios.\n",
       "53                                                               ICML-2023-Generating-Novel,-Designable,-and-Diverse-Protein-Structures.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                To ensure fairness, we retrain ProtDiff and Fold-\\ningDiff on our filtered SCOPe dataset.\n",
       "54                                                               ICML-2023-Better-Diffusion-Models-Further-Improve-Adversarial-Training.pdf                                                                                                                                                                                                                                                              In addition to leveraging external\\ndatasets or generated data, several enhancements for AT\\nhave been made employing strategies inspired by other ar-\\neas, including metric learning (Mao et al., 2019; Pang et al.,\\n2020a;b), self-supervised learning (Chen et al., 2020a;b;\\nNaseer et al., 2020; Wang & Liu, 2022), ensemble learn-\\ning (Tram `er et al., 2018; Pang et al., 2019), fairness (Ma\\net al., 2022; Li & Liu, 2023), and generative modeling (Jiang\\net al., 2018; Wang & Yu, 2019; Deng et al., 2020).\n",
       "55                                                       ICML-2023-Large-Language-Models-Can-Be-Easily-Distracted-by-Irrelevant-Context.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  (2022) evaluate\\nvarious large language models under several metrics, in-\\ncluding accuracy, robustness, fairness, etc.\n",
       "56                                                              ICML-2023-Cramming-Training-a-Language-Model-on-a-Single-GPU-in-One-Day.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             It is unclear whether other properties that have been extensively\\nstudied in the literature for the existing public BERT checkpoint, such as robustness, out-of-distribution generalization,\\nfairness or inherent biases, carry over to the crammed model."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the keyword to search for\n",
    "keyword_to_search = \"Fairness\"  # Replace with the keyword you want to search for\n",
    "\n",
    "# Initialize lists to store results\n",
    "paper_names = []\n",
    "flagged_sentences = []\n",
    "\n",
    "# Process NeurIPS papers\n",
    "if keyword_to_search in sentences_neurips:\n",
    "    for filename in os.listdir(\"Paper Neurips\"):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(\"Paper Neurips\", filename)\n",
    "            text = extract_text_excluding_references(pdf_path)\n",
    "            keyword_counts, keyword_sentences = count_keywords(text, [[keyword_to_search]])\n",
    "            if keyword_to_search in keyword_sentences:\n",
    "                for sentence in keyword_sentences[keyword_to_search]:\n",
    "                    paper_names.append(filename)\n",
    "                    flagged_sentences.append(sentence)\n",
    "\n",
    "# Process ICML papers\n",
    "if keyword_to_search in sentences_icml:\n",
    "    for filename in os.listdir(\"Paper ICML\"):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(\"Paper ICML\", filename)\n",
    "            text = extract_text_excluding_references(pdf_path)\n",
    "            keyword_counts, keyword_sentences = count_keywords(text, [[keyword_to_search]])\n",
    "            if keyword_to_search in keyword_sentences:\n",
    "                for sentence in keyword_sentences[keyword_to_search]:\n",
    "                    paper_names.append(filename)\n",
    "                    flagged_sentences.append(sentence)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "df_flagged_sentences = pd.DataFrame({\n",
    "    \"Paper Name\": paper_names,\n",
    "    \"Flagged Sentence\": flagged_sentences\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_flagged_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence extraction without paper names: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_to_search = \"User influence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['However, user control-\\nlability of the generated image, and fast adapta-\\ntion to new tasks still remains an open challenge,\\ncurrently mostly addressed by costly and long re-\\ntraining and fine-tuning or ad-hoc adaptations to\\nspecific image generation tasks.',\n",
       " 'Recently,\\na surge of methods have been proposed to gain wider and\\nbetter user controllability.',\n",
       " 'However, in\\ncontrast to existing works that target a specific application,\\nwithout a well defined objective, we propose a more general\\napproach that allows us to unify different user control inputs\\nin a more principled manner.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_icml[keyword_to_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abstract\\nAttaining a high degree of user controllability in visual generation often requires\\nintricate, fine-grained inputs like layouts.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_neurips[keyword_to_search]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
