{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To Transformers and Beyond: Large Language\n",
      "Models for the Genome\n",
      "Micaela E. Consens1, 2, 3, Cameron Dufault1, Michael Wainberg4, Duncan Forster2, 5, 6,\n",
      "Mehran Karimzadeh2, 7, 8, 9, Hani Goodarzi7, 8, 9, Fabian J. Theis10, 11, 12, 13, Alan Moses1, 14,\n",
      "and Bo Wang1, 2, 3, 15*\n",
      "1Department of Computer Science, University of Toronto, Toronto, Ontario, Canada\n",
      "2Vector Institute for Artificial Intelligence, Toronto, Ontario, Canada\n",
      "3Peter Munk Cardiac Center, University Health Network, Toronto, Ontario, Canada\n",
      "4Prosserman Centre for Population Health Research, Lunenfeld-Tanenbaum Research Institute, Toronto, Ontario,\n",
      "Canada\n",
      "5Department of Molecular Genetics, University of Toronto, Toronto, Ontario, Canada\n",
      "6The Donnelly Centre, University of Toronto, Toronto, Ontario, Canada\n",
      "7Department of Biochemistry & Biophysics, University of California, San Francisco, San Francisco, California, USA\n",
      "8Department of Urology, University of California, San Francisco, San Francisco, California, USA\n",
      "9Helen Diller Family Comprehensive Cancer Center, University of California, San Francisco, San Francisco,\n",
      "California, USA\n",
      "10Institute of Computational Biology, Department of Computational Health, Helmholtz Munich, Munich, Germany\n",
      "11TUM School of Life Sciences Weihenstephan, Technical University of Munich, Munich, Germany\n",
      "12Department of Mathematics, School of Computation, Information and Technology, Technical University of Munich,\n",
      "Garching, Germany\n",
      "13Munich Center for Machine Learning, Technical University of Munich, Garching, Germany\n",
      "14Department of Cell and System Biology, University of Toronto, Toronto, Ontario, Canada\n",
      "15Department of Laboratory Medicine and Pathobiology, University of Toronto, Toronto, Ontario, Canada\n",
      "*e-mail: bowang@vectorinstitute.ai\n",
      "ABSTRACT\n",
      "In the rapidly evolving landscape of genomics, deep learning has emerged as a useful tool for tackling complex computational\n",
      "challenges. This review focuses on the transformative role of Large Language Models (LLMs), which are mostly based on the\n",
      "transformer architecture, in genomics. Building on the foundation of traditional convolutional neural networks and recurrent\n",
      "neural networks, we explore both the strengths and limitations of transformers and other LLMs for genomics. Additionally, we\n",
      "contemplate the future of genomic modeling beyond the transformer architecture based on current trends in research. The\n",
      "paper aims to serve as a guide for computational biologists and computer scientists interested in LLMs for genomic data. We\n",
      "hope the paper can also serve as an educational introduction and discussion for biologists to a fundamental shift in how we will\n",
      "be analyzing genomic data in the future.\n",
      "Introduction\n",
      "In the past decade, deep learning has gone from a niche technology to a tool that has been successfully applied to a diverse\n",
      "range of tasks, from generating art1,2, language representation3–6, and even to predicting protein structure from amino acid\n",
      "sequence7. A common feature driving the success of deep learning tools at these tasks is the rapidly expanding size of available\n",
      "datasets, along with their improved accessibility and increasingly multi-modal nature. Perhaps just as important has been the\n",
      "push to generate larger and larger models8. At the same time, novel techniques for capturing genomic information9,10such as\n",
      "chromatin accessibility11,12, methylation12,13, transcriptional status14,15, chromatin structure16and bound molecules12have\n",
      "provided a large and varied source of omics data to mine17.\n",
      "The prevalence of deep learning tools applied to genomics in recent years is a natural consequence of more available\n",
      "and varied genomic data sources, as well as the introduction of larger deep learning models. Deep learning tools provide\n",
      "a solution to the computational and data analysis challenges posed by the increasing scale and variety of omics datasets18.\n",
      "One of the key areas that deep learning has been applied to within the field of genomics is taking as input DNA sequences,\n",
      "RNA sequences, or single-cell RNA sequencing data and predicting missing high-dimensional modalities corresponding toarXiv:2311.07621v1  [q-bio.GN]  13 Nov 2023\n",
      "the same data. For example, using DNA sequence, these models can predict regulatory annotations like transcription-factor\n",
      "binding, RNA-binding, chromatin accessibility, contact-maps, gene expression, RNA-seq coverage, promoter/enhancer region\n",
      "identification and more19–39. Using single-cell RNA sequencing data, these models can annotate cell types, correct batch effects,\n",
      "make gene dosage sensitivity predictions, among other tasks40,41.\n",
      "Deep learning models have evolved over time to address the growing complexities of genomic data and the nuances of\n",
      "the many high-dimensional modalities available to measure the genome. For the last decade deep learning models on DNA\n",
      "sequence have been dominated by convolutional neural network (CNN) structures20–26,29,32. At the same time, deep learning\n",
      "models making predictions from single-cell RNA sequencing data have been dominated by different forms of autoencoders42,43.\n",
      "However, as the popularity of transformer models44in the fields of computer vision and natural language processing (NLP),\n",
      "among others, has increased, so too has their application to genomic modelling problems30,31,39–41,45,46. Transformer models\n",
      "have revolutionized NLP and various other fields, though they were initially conceived for sequence-to-sequence problems.\n",
      "Their applicability extends to genomic sequences, even being adaptable for tasks where the final goal of the model is not\n",
      "to produce a sequence but output a quantitative assay or classification31,46. Furthermore, transformers applied to genomic\n",
      "data offer a novel conceptual framework, the attention mechanism , to study the organization and grammar of the genome.\n",
      "Simultaneously, in the field of machine learning, computational advances continue to be made in improving the efficiency of the\n",
      "transformer, allowing the size of these models to increase along with their predictive power47,48. This will further accelerate the\n",
      "application of deep learning models for genomics, where methodologies for improving transformer efficiency are now being\n",
      "adopted in newer models, and most recently, genomic models are being proposed with novel architectures that claim to be the\n",
      "“next transformer”49.\n",
      "This review will discuss the trajectory of deep learning approaches in genomics, with a detailed discussion on the application,\n",
      "successes, and challenges of Large Language Models (LLMs) of the genome. LLMs, as referred to here, are models pre-trained\n",
      "on sequences of biological tokens such as DNA, k-mers, or gene identifiers. We adopt the term LLM, traditionally used in\n",
      "NLP, due to the pre-training approach analogous to that for text data. Although numerous review papers have explored deep\n",
      "learning models in genomics, with topics spanning from general introductions to specific discussions on model interpretation,\n",
      "understanding gene regulation, predicting the impact of genetic variation, and unveiling new applications18,50–60, we take a\n",
      "different approach. The paper will focus exclusively on the application to genomics of transformer-hybrid models, transformer-\n",
      "based LLMs, and other LLMs. Although non-transformer LLMs currently represent a nascent and less-explored niche, we\n",
      "believe that the exploration of architectures beyond traditional attention mechanisms will soon accelerate. The early research\n",
      "in this space holds promising and innovative prospects for the field. We will not discuss the use of transformers or language\n",
      "models for protein sequences or review non-LLM models used for single-cell transcriptomics as these topics have been reviewed\n",
      "elsewhere42,43,61.\n",
      "In this paper, we first provide an overview of the transformer architecture, as well as a briefing on the Hyena layer, within\n",
      "the context of genomics. This is followed by a succinct review of previous deep learning architectures used in this field,\n",
      "establishing a foundation for an examination of novel architectures since. We introduce transformer-hybrid models, including\n",
      "transformers to predict assay data, followed by transformer-based LLMs and other LLMs applied to the genome. This review is\n",
      "intended for computational biologists with deep learning experience interested in understanding LLMs for the genome, as well\n",
      "as for computer scientists who are keen on gaining insights into the research opportunities within this exciting field.\n",
      "Glossary\n",
      "attention A mechanism in transformers that dynamically weighs the pairwise importance of different input elements in a\n",
      "sequence, or their features, based on the attention calculation. The dot-product attention calculation consists of a linear\n",
      "projection of the inputs to the transformer module into keys, queries and values, where the attention matrix is the result\n",
      "of a softmax applied on the dot product between keys and queries scaled by the dimension of the keys. The final attention\n",
      "calculation involves taking the weights obtained from the attention matrix and computing the dot product with the values.\n",
      "CNN A convolutional neural network. This network has a series of convolutional layers built upon interconnected neuron\n",
      "subsets, or convolutions. Here, neurons that are spatially proximate are linked, while those distant are not (see\n",
      "Supplemental Figure 4 (b)). These nearby neurons share the weight, or connection strength, which is collectively updated\n",
      "during the backpropagation process. The degree of neuronal proximity dictating their connections is controlled by a\n",
      "hyperparameter known as the “kernel size”. In genomics, these kernels convolve over DNA sequence, represented\n",
      "by a one-hot encoding of the A T G C nucleotides (sometimes including an “N” nucleotide), and learn to recognize\n",
      "specific subsequences or motifs. In DNA, these kernels are 1D convolutions over 4-channels (or 5, if “N” is included)\n",
      "representing the four nucleotides. This can be conceptualized similarly to convolutions applied over a coloured image,\n",
      "where 2D kernels convolve over 3-channels (Red Green Blue, or RGB, channels). These kernels compute the sum of\n",
      "2/33\n",
      "elementwise multiplication between the one-hot encoding of DNA and the filter, to create pattern matches, or motifs,\n",
      "which can later be compared to experimentally determined motifs.\n",
      "decoder The segment of the transformer that produces a sequence output, often based on the representation generated by the\n",
      "encoder. In the context of genomics, this decoder can take representations of DNA sequences that were learned by the\n",
      "encoder and reconstruct a desired output sequence.\n",
      "DFNN A deep feed-forward neural network. This is the earliest deep learning model architecture. A fully-connected DFNN\n",
      "has a series of densely connected layers such that every neuron in a given layer maintains connections to all neurons in\n",
      "the succeeding layer. This property allows for precise modulation of the connection strength between each neuron during\n",
      "the backpropagation process, as visualized in Supplemental Figure 4 (a) .\n",
      "encoder The part of the transformer that processes and represents input data. In the context of genomics, the encoder takes\n",
      "DNA sequences and creates a numeric representation of the features of this sequence, which can later be used by the\n",
      "decoder to make predictions.\n",
      "fine-tuning Adjusting a pre-trained model to adapt it for a particular application through training it on a smaller, task-specific\n",
      "dataset. Sometimes this involves “freezing” the majority of the model’s weights learned in pre-training and only adjusting\n",
      "a smaller number of weights during fine-tuning, as opposed to all trainable weights.\n",
      "fully-connected layer Also known as a densely connected layer. This is a neural network layer where each neuron is connected\n",
      "to every neuron in the previous and following layer. A fully-connected layer is often used as the final layer in a neural\n",
      "network to integrate learned features into predictions, leveraging its capacity to consider all feature combinations, which\n",
      "is critical for tasks like classification or regression.\n",
      "kernel The filter used in convolutional layers. In the context of genomics, it computes the sum of elementwise multiplication\n",
      "between itself and the one-hot encoding of DNA to create pattern matches, or motifs, which can later be compared to\n",
      "experimentally determined motifs. The greater the value of the sum over the elementwise multiplication between kernel\n",
      "and sequence, the greater the concordance between the kernel and sequence. Kernels with high values in positions that\n",
      "match up to the sequence encoding, and negative values in all other positions (to penalize differences), can capture\n",
      "subsequences very effectively.\n",
      "multi-head attention A variation on attention where instead of the keys, queries and values used to calculate attention coming\n",
      "from a single linear projection of each of the inputs, the inputs are linearly projected into keys, queries and values h\n",
      "times, where his the number of heads in the multi-headed attention. This means the attention calculation is calculated in\n",
      "parallel across all hheads, to be concatenated and once again projected, and ideally hdifferent linear projections of the\n",
      "inputs are learned. Multi-head attention can be self-attention or attention between an encoder and decoder.\n",
      "pre-training Training a model on a large dataset that is not necessarily task-specific, leveraging more data to learn underlying\n",
      "patterns or structures within the data. Later the model can be fine-tuned for specific tasks using a smaller well-curated\n",
      "dataset. Unsupervised pre-training is a form of pre-training whereby a large corpus of unlabelled data is used to\n",
      "pre-train the model, requiring only a small labelled dataset to be fine-tuned. This allows for available but unlabelled\n",
      "data to contribute to model performance, and improve the results the model would have on an otherwise limitingly small\n",
      "dataset.\n",
      "pretext A training task for deep learning models, typically transformers, during unsupervised pre-training. These tasks are\n",
      "not the final tasks the model will be evaluated on, but instead work to help the model gain an understanding of the\n",
      "organization and grammar of the genome (in the context of genomics). Commonly used pretext tasks are Masked\n",
      "Language Modelling (MLM) and Autoregressive Language Modelling (ALM)/Next Token Prediction (NTP).\n",
      "RNN A recurrent neural network. RNNs are specialized neural networks often equipped with “memory” that allows the model\n",
      "to capture temporal or spatial dependencies within the sequence data (see Supplemental Figure 4 (c)). RNNs include\n",
      "Long Short Term Memory Networks (LSTMs), Bi-directional LSTMs (BLSTMs) and Gated Recurrent Units (GRUs). An\n",
      "RNN operates on data sequentially, where each sequence element is subjected to the same computational process. Each\n",
      "computation requires a new sequence element and the “memory” of previous elements, yielding an updated “memory”\n",
      "state that is continuously propagated throughout the sequence.\n",
      "3/33\n",
      "self-attention A specific form of attention that is calculated within the layers of either the encoder or the decoder of a\n",
      "transformer, not between these two modules. This means the inputs projected into keys, queries and values all come from\n",
      "the encoder, or all come from the decoder.\n",
      "Transformers\n",
      "To understand the use of transformers in the context of genomic modelling requires both a foundational understanding of the\n",
      "architecture and its usual training regime. In explaining the transformer, we assume readers have a grasp of several prominent\n",
      "concepts in machine learning, including the common layer types of deep feed-forward neural networks (DFNNs), convolutional\n",
      "neural networks (CNNs), and recurrent neural networks (RNNs). For a comprehensive introduction to deep learning, specifically\n",
      "in the context of genomics, we direct readers to the prominent review papers of Zou et al.18, Jones et al.50, Min et al.52, and\n",
      "Wainberg et al.54. If not already familiar, we encourage readers to familiarize themselves with DFNNs, CNNs63, and RNNs64,65\n",
      "using the existing literature. The glossary additionally provides concise definitions.\n",
      "Architecture\n",
      "The transformer architecture, introduced in 2017, features layers of stacked self-attention mechanisms, typically multi-head\n",
      "attention , alongside addition and normalization layers, skip-connections, and fully-connected layers for final output predictions.\n",
      "We’ll first explore the historical application of these components in deep learning for genomics before examining their\n",
      "integration within the transformer module as a “meta-layer”. Following this, we’ll outline the original presentation of the\n",
      "transformer as an encoder-decoder framework44, comprising stacked transformer-encoder modules and transformer-decoder\n",
      "modules. Finally, we’ll review the various transformer model types applicable to genomics, including encoder-decoder\n",
      "transformers, encoder-only transformers, and decoder-only transformers.\n",
      "Attention\n",
      "The first iterations of the attention mechanism were employed with Recurrent Neural Networks (RNNs), not in the context of\n",
      "transformers. However, computational efficiency was becoming a bottleneck for RNNs due to hardware constraints which\n",
      "prevented RNN-based models from expanding their parameter number44. The newly introduced attention mechanism of the\n",
      "transformer model eschewed recurrence, instead relying entirely on attention to draw global dependencies. The transformer’s\n",
      "lack of recurrence was optimized for the attention mechanism to be computed in parallel on GPU hardware, overcoming the\n",
      "RNNs computational limitations.\n",
      "In the transformer, self-attention , or the use of attention to relate different positions within a single sequence to construct a\n",
      "representation of that sequence, is used to compute representations of input and output sequences. The transformers attention\n",
      "allows them to account for relationships within sequences that are not determined by immediate proximity, like RNNs, while\n",
      "leveraging parallelization on GPUs for computational efficiency, unlike RNNs66. The key intuition in employing transformers\n",
      "for genomic sequences is their capacity to attend to long-range interactions within DNA sequences, which can span tens of\n",
      "thousands of base pairs. However, transformers still experience computational memory constraints. Direct base-to-base attention\n",
      "over very long-range genomic sequences (such as millions of base pairs) has not yet been achieved using transformers30,31,45.\n",
      "Instead, in genomics, attention is often applied to features extracted by dilated convolutions before transformer modules,\n",
      "thereby extending the range of self-attention calculations, at the cost of losing pairwise scores of relevance across a whole\n",
      "sequence31,45,62.\n",
      "Commonly used attention functions are calculated additively, or multiplicatively on sequences, where the latter case is\n",
      "referred to as scaled dot-product attention and is computed as follows44:\n",
      "attention (Q,K,V) =so ftmax\u0012QKT\n",
      "√dk\u0013\n",
      "V\n",
      "Where the input sequence is transformed into the queries ( Q), keys ( K) in dimension dk, and values ( V) in dimension\n",
      "dv. Here, queries represent the information at each position in the sequence, keys symbolize the associated information that\n",
      "the model should attend to, and values convey the information each position forwards to positions attending to it31. This\n",
      "setup facilitates the computation of the attention score as a weighted sum of all values, with the weight of each value being\n",
      "determined by the compatibility between the query and the corresponding key. The attention mechanism in transformers for\n",
      "genomic applications that use encoder-only transformers involves only “self-attention”. “Attention” is calculated between the\n",
      "encoder-decoder, and as transformer models for genomics usually include only the encoder of the transformer model, there is\n",
      "usually only encoder self-attention. In these self-attention modules, queries ( Q), keys ( K), and values ( V) all originate from the\n",
      "previous encoder layer, which ensures each position attends to all other positions in the prior layer.\n",
      "4/33\n",
      "Figure 1. A big picture look at the power of Genome LLMs. These models can take in both sequential (for example, DNA\n",
      "sequence but also ATAC-seq, Hi-C etc.) and non-sequential (for example, single-cell RNA-seq but also bulk transcriptome,\n",
      "multiome etc.) data and extract signals to make predictions on functional regions, disease-causing SNPs (single nucleotide\n",
      "polymorphisms or variations at a single position in a DNA sequence among individuals), gene expression predictions, and\n",
      "more. The training phase allows LLMs to learn the underlying structure of a dataset, and during the adaptation process\n",
      "(fine-tuning or prompting, etc.) the downstream task performance is evaluated. In the tokenization process different data\n",
      "sources (sequential vs. non-sequential) are prepared for input to the genome LLM differently. Sequence data is often tokenized\n",
      "using k-merization. In contrast, non-sequential data like single-cell RNA-seq data can undergo a more complicated\n",
      "tokenization scheme, where data can be represented using geneIDs as tokens, or some tokenization of gene expression values or\n",
      "rankings. LLMs often use the Masked Language Modelling pretext task in pre-training, where tokens are masked at different\n",
      "positions in the sequence. The model can use information from preceding and succeeding tokens to predict the masked token\n",
      "(as represented by the arrows indicating information flow available to the model). However, in the Autoregressive Language\n",
      "Modelling (ALM) pretext task the model sees a series of “known” tokens before being having to predict the next token. This\n",
      "means the model can only use preceding information (“known” tokens) to make a prediction.\n",
      "5/33\n",
      "Figure 2. Transformer-LLMs and Transformer-Hybrids. Transformer-LLM models can take in tokenized data (either\n",
      "unlabelled sequence data or non-sequential data like single-cell RNA-seq data). The figure shows an example of k-merized\n",
      "DNA data, where k=6. Once tokenized, input data is immediately passed to a transformer module, the meta-layer combination\n",
      "of add-and-norm layers, fully-connected layers and the attention mechanism30,40,46. In the figure, we focus on the attention\n",
      "mechanism within the transformer layer, outlining the steps involved in multi-headed attention. The keys, queries, and values in\n",
      "the attention mechanism are projections of the input sequence and attention is calculated across the inputted sequence at\n",
      "token-level resolution. Final predictions can be classification or identification tasks. The transformer-hybrid model takes in\n",
      "labelled data which is then one-hot encoded. Oftentimes, it is then passed through a series of convolutional layers, sometimes\n",
      "including dilated convolutions, before eventually reaching the transformer module. Here we represent the transformer module\n",
      "as a golden block, showing the attention mechanism is applied within these blocks. Transformer-hybrid models make\n",
      "predictions of experimental assays, either quantitative predictions like ChIP-seq intensities, or binary predictions like the\n",
      "presence/absence of ChIP-seq peaks31,62.\n",
      "6/33\n",
      "Many transformer models employ a multi-headed attention mechanism, which facilitates information sharing across a\n",
      "sequence using fewer layers than vanilla self-attention67. The attention calculation is modified as follows for hattention heads\n",
      "with dmodel -dimensional queries, keys and values44:\n",
      "multi _head (Q,K,V) =concat (head 1,...,head h)WO\n",
      "where\n",
      "head i=attention (QWQ\n",
      "i,KWK\n",
      "i,VWV\n",
      "i)\n",
      "Where WQ\n",
      "i∈Rdmodel×dk,WK\n",
      "i∈Rdmodel×dk,WV\n",
      "i∈Rdmodel×dvandWO∈Rhdv×dmodel. We can see the use of matrices of size k×n\n",
      "for each head shows that multi-head attention is a low-rank approximation for the full n×nattention matrix.\n",
      "Multi-head attention creates multiple different representations of the input to the transformer block through the learned\n",
      "representations of the inputs known as queries ( Q), keys ( K), and values ( V). Instead of a single transformation of the input\n",
      "into queries, keys and values, the input is transformed into queries, keys and values for each of the hattention heads in a\n",
      "separate calculation for each head. This means there are hlearned representations of queries, keys and values, or hlearned\n",
      "linear projections to dk,dkanddvdimensions, respectively. The attention function is then calculated in parallel (very efficiently\n",
      "on GPU) for each of these learned representations to produce dv-dimensional output values. This approach lends more stability\n",
      "to the model during training and enhances modeling of long-range interactions67. The value in multi-head attention is that it\n",
      "allows the model to jointly attend to information from different representation subspaces at different positions, as long as the\n",
      "different attention heads focus on distinct features, i.e. as long as the learned representations are not all the same68.\n",
      "Add-and-norm Layers, Skip-Connections and Fully-Connected Layers\n",
      "In transformers applied for genomics, add-and-norm layers perform a layer normalization and add in the original embeddings\n",
      "of a DNA sequence (for example) through a skip-connection. Normalization layers have long been used in deep learning as a\n",
      "way to prevent over-fitting, improve generalisation performance and accelerate convergence (a state during training in which\n",
      "the model loss becomes stable)69. Skip-connections were initially proposed as a solution to the Vanishing Gradient Problem, an\n",
      "artefact of backpropagation where the loss signal degrades proportionally to the depth of a network70. These layers increase the\n",
      "relationship between output signal and earlier layers in the model by providing connections between these layers.\n",
      "Skip-connections have been widely used in genomics, such as in the Akita model36. Akita was built by modifying the\n",
      "CNN-based Basenji architecture for the task of Hi-C contact domain prediction16,36. The Akita model used dilated residual\n",
      "convolutions on DNA sequence data, introducing skip-connections, also known as residual connections, between successive\n",
      "dilated residual convolutional layers. Skip-connections used between dilated residual convolutions, as in Akita, allow for a\n",
      "model to learn to skip certain dilated convolutions across sequence if they do not contribute towards accurate prediction and\n",
      "instead hinder accuracy.\n",
      "Similarly, within transformers, skip-connections are used to solve the Vanishing Gradient Problem and, like in CNNs, have\n",
      "additional benefits. By encouraging the addition of the original embeddings through the skip-connection in the add-and-norm\n",
      "layer, information flow within the transformer mechanism is localized within the transformer block. While the self-attention\n",
      "mechanism continues to dynamically expand context based on relevance, the skip-connections remind the transformer what the\n",
      "original state was, ensuring the contextual representations of input tokens remain attached to those tokens.\n",
      "In transformers, fully-connected layers are used at the end of a transformer block to summarize information within a block.\n",
      "This information is either summarized for it to be sent to the next transformer block, or to be used for a final output.\n",
      "Encoder-decoders\n",
      "Encoder-decoder frameworks are used outside of transformer architectures, and have found applications in genomics. The\n",
      "Orca71paper leveraged an encoder-decoder framework composed of a one-dimensional (1-D) CNN encoder and a two-\n",
      "dimensional (2-D) CNN decoder to predict 3-D contact maps of the genome. The 1-D CNN encoder takes in a 1-D DNA\n",
      "sequence, and the 2-D CNN decoder outputs a 2-D contact map to make predictions of genomic proximity from sequence alone.\n",
      "The encoder-decoder framework proved particularly useful in the Orca71paper, as using the encoder-decoder structure does not\n",
      "constrain input and output sequences to be the same length, accommodating the complexity and variability inherent in genomic\n",
      "data.\n",
      "However, using a CNN-structure in an encoder-decoder framework, as used by Orca71, limits the context window of a\n",
      "model to a predetermined kernel, or filter size. All encoded and decoded relationships are determined by the filter size applied\n",
      "to the sequence in the encoder and decoder. In transformers, however, the attention mechanism has potential to overcome this\n",
      "limitation. As long as the inputted sequence has full dense attention across it, all pair-wise relationships are calculated between\n",
      "sequence elements based on context.\n",
      "7/33\n",
      "The original transformer model was introduced as an encoder-decoder framework. The transformer-encoder is designed to\n",
      "encode input sequences into fixed-size context-aware representations, and the transformer-decoder produces variable-length\n",
      "sequences based on this representation and its own input. While the transformer modules in the encoder and decoder are very\n",
      "similar, the original transformer-decoder has an extra attention layer to focus on the encoder’s output44.\n",
      "Encoder-only and Decoder-only Transformers\n",
      "Transformer models are not always encoder-decoders. Sometimes, transformer models can be used just as encoders, or just\n",
      "as decoders. In NLP, the Bidirectional Encoder Representations from Transformers (BERT)3and the Generative Pre-trained\n",
      "Transformer (GPT)4are two distinct frameworks for implementing transformers. BERT3is an encoder-only transformer\n",
      "framework with 12 transformer layers, each with 12 attention heads, usually pre-trained using a Masked Language Modelling\n",
      "(MLM) pretext task. MLM is a pretext training task where the model is pre-trained to predict a word that’s been masked out of a\n",
      "sentence. The GPT framework4is decoder-only, and is pre-trained with an Autoregressive Language Modelling (ALM) pretext\n",
      "task. ALM is a pretext training task where the model is pre-trained to predict the next word in a sequence given the previous\n",
      "words. In NLP, GPT models generate text unidirectionally and sequentially, predicting the next word based on the previous one.\n",
      "BERT, in contrast, is designed for bidirectional representation, processing words from both left-to-right and right-to-left. In\n",
      "genomics, a GPT-style model would use preceding nucleotide information to make a prediction about subsequent nucleotides in\n",
      "a sequence. A BERT-style model could predict missing nucleotide information within a sequence using both preceding and\n",
      "succeeding nucleotides.\n",
      "BERT-based and GPT-based models work best for different tasks3,6. Encoder-only models, like BERT, are useful in\n",
      "the cases where final predictions have high accuracy when based off only an embedding, or feature representation of the\n",
      "inputted sequence. This is oftentimes true for classification tasks. BERT-based models pre-trained with MLM are useful for\n",
      "understanding genomic sequences where the overall context (both upstream and downstream) is important, such as identifying\n",
      "genomic features or classifying sequences where directionality is not critical. In genomic modelling, the BERT framework\n",
      "has already been applied. DNABERT30, as the model is called, can accurately identify whether DNA sequences are TATA or\n",
      "non-TATA promoters directly from the final embedding of the sequence alone.\n",
      "In NLP, decoder-only transformers pre-trained with ALM, like GPT-3 and GPT-46,8are appropriate for tasks that involve\n",
      "predicting sequences where directionality is important. This includes modeling anything co-transcriptional or co-translational,\n",
      "such as RNA splicing or protein folding, where these sequences are biologically synthesized in a unidirectional manner.\n",
      "Decoder-only models in general provide the best zero-shot generalisation from purely unsupervised pre-training72. Zero-shot\n",
      "generalisation is a model’s performance on tasks the model was not explicitly trained on, and unsupervised pre-training is a\n",
      "specific training regime where models are trained on large amounts of unlabelled data through a pretext task. Models exhibiting\n",
      "good zero-shot performance after pre-training on DNA sequences could capture multiple aspects of genomic grammar and\n",
      "structure instead of, for example, focusing on learning the best representation of a DNA sequence for predicting TATA-promoter\n",
      "regions (which may be too-constrained a task for good generalisation beyond this task). However, decoder-only models have\n",
      "yet to be widely adopted for transformers in genomics. A GPT-style model was implemented for single-cell omics data by\n",
      "scGPT40, but it was not a true decoder-only model. The decoder-only framework for genomics has so far only been used by a\n",
      "model not based on the transformer architecture, HyenaDNA49.\n",
      "Training\n",
      "The attention mechanism within transformers is often attributed as the main reason for their success68,73. However, perhaps just\n",
      "as important as the attention mechanism, is the transformer’s capacity to be pre-trained73. While pre-training is not restricted to\n",
      "transformers, and has been shown to benefit other architectures immensely74,75, transformers are the most commonly pre-trained\n",
      "architecture.\n",
      "In the field of genomics, as is the case in many areas of study, the availability of unlabelled data usually vastly exceeds that\n",
      "of labelled data27. The transformer model, along with some others, can address this imbalance by undertaking a pre-training\n",
      "phase, where it can learn in an unsupervised orself-supervised manner from a large unlabelled dataset through a pretext\n",
      "task. This pre-training stage effectively exploits the abundant unlabelled data available to learn general data representations.\n",
      "The model can then be fine-tuned for a specific task using a smaller labelled dataset, thereby benefiting from both the broad\n",
      "generalization capabilities gained during pre-training and the task-specific adaptations acquired during fine-tuning73.\n",
      "Pre-Training\n",
      "Pre-training can be unsupervised, supervised, or semi-supervised. Pre-training is an initial phase of training where a model, in\n",
      "this case a transformer, learns from a large corpus of data. When this data is unlabelled, and there is not a specific downstream\n",
      "task the model is being pre-trained for, the pre-training is called unsupervised. When the data has labels, the pre-training is\n",
      "considered supervised, and when both labelled and unlabelled data is used, the pre-training is considered semi-supervised.\n",
      "8/33\n",
      "Supervised pre-training can sometimes be preferred. This method can capture features that are more relevant to specific\n",
      "tasks a model will later be fine-tuned on, reducing the computational cost of fine-tuning. Supervised pre-training can sometimes\n",
      "lead to faster convergence or better performance on the downstream target task. Additionally, when there is a large amount of\n",
      "labelled data available for pre-training, it may not be necessary to use unsupervised pre-training.\n",
      "Semi-supervised pre-training combines the benefits of both supervised and unsupervised learning, and their drawbacks as\n",
      "well. This approach can be beneficial when there is some amount of labelled data and complimentary unlabelled data available.\n",
      "While each of these approaches to pre-training has their own strengths and limitations, unsupervised pre-training is an\n",
      "important direction for future research. This is especially true in biology, where much of the inherent signal in available\n",
      "data is not yet well enough understood to be labelled76. An unsupervised learning process allows a model to understand and\n",
      "capture general patterns and representations from the input data, independent of any specific downstream task. Additionally,\n",
      "this unsupervised approach potentially encourages the model to uncover signals in the data that researchers are unaware of.\n",
      "The broad pretext task assigned during the unsupervised pre-training phase, such as Masked Language Modelling (MLM)\n",
      "or Autoregressive Language Modeling (ALM)/Next Token Prediction (NTP), facilitates the model’s learning of meaningful\n",
      "data representations despite not having labels. In the context of genomics, pre-training should expose the model to diverse\n",
      "genomic sequences, enabling it to understand various sequence patterns, context relations, and general nuances of genomic\n",
      "data. Pre-training results in a set of pre-trained weights that reflect these learned representations. These weights can then be\n",
      "updated during the subsequent fine-tuning phase as required.\n",
      "Masked-Language Modelling\n",
      "MLM is a pretext task for the pre-training process. This technique involves randomly masking some of the input tokens, with\n",
      "the model’s task being to predict these masked tokens. In the context of genomics, the ‘tokens’ could be the individual bases in\n",
      "the DNA sequence. By masking out specific nucleotides within the genome and having the model “fill in the gaps”, important\n",
      "relations between nucleotides are learned. The objective of this technique is to enable the model to understand the contexts\n",
      "in which each base can occur, thus gaining a holistic understanding of the genomic sequence. This exposure to a diverse set\n",
      "of sequences enhances the model’s capacity to recognize different genomic sequence patterns and contexts. This is the most\n",
      "common pre-training task in genomics30,39,41,46,77.\n",
      "Autoregressive Language Modeling\n",
      "Autoregressive Language Modeling or Next Token Prediction is a training technique used in models such as Generative\n",
      "Pretrained Transformer or GPT4,6, where the task is to predict the next token in a sequence based on all previously observed\n",
      "tokens. In NLP, this usually means predicting the next word in a sentence. For genomic sequences, this could translate into\n",
      "predicting the next base in a DNA sequence given the preceding bases49.\n",
      "This training regime allows the model to learn the probability distribution of a sequence and use this learned distribution to\n",
      "generate sequences. In the context of genomics, this could involve the model learning the probability distribution of bases in a\n",
      "DNA sequence, and then using this learned distribution to predict the likely continuation of a given DNA sequence.\n",
      "An artefact of this training is unidirectionality, meaning the model only considers previous, or “known”, tokens in the\n",
      "sequence when predicting the next one. In contrast, bidirectional models based on BERT, like DNABERT30use MLM. They\n",
      "consider both previous and future tokens when predicting a masked token. As such, ALM and MLM provide different types of\n",
      "sequence understanding and can be useful for different types of tasks in genomics.\n",
      "One key benefit of ALM in the context of genomics lies in sequence generation tasks, such as predicting the potential\n",
      "continuation of a partially known sequence. However, its unidirectional nature means it might be less effective at understanding\n",
      "the broader context of a sequence than bidirectional models based on BERT3,30, which consider both previous and subsequent\n",
      "tokens in the sequence. As such, the best choice of pre-training regime depends on the specifics of the task at hand. To the\n",
      "best of our knowledge, this type of pretext task has only been applied to the scGPT40model in the context of transformers for\n",
      "genomics, and the HyenaDNA49model for the Hyena layer in genomics.\n",
      "Fine-tuning\n",
      "Once the model is pre-trained, it undergoes a second training phase known as fine-tuning, a form of transfer learning. This\n",
      "process involves training the model on a smaller, task-specific dataset that is usually labelled. The pre-trained model parameters\n",
      "are adjusted (or “fine-tuned”) during this phase, allowing the model to specialize in the target task. In fine-tuning, model\n",
      "weights from earlier layers learned in pre-training can be frozen, with only later layers adjusting their weights according to\n",
      "the loss function in the fine-tuning stage. Even if the previous weights are not frozen, it is expected that the model weights\n",
      "will not change very dramatically from the pre-training, only as much as is needed to accomplish the more specific task.\n",
      "Unfortunately, this is usually left as an assumption. The pre-training regimes of many genomic models are not evaluated closely\n",
      "to determine what the model is learning in pre-training vs. fine-tuning78. In theory, the fine-tuning process benefits from the\n",
      "broad representations learned during pre-training, enabling the model to effectively adapt to the specific task with less labelled\n",
      "9/33\n",
      "data. For instance, in genomics, the unsupervised pretext task learned during pre-training could be MLM on the entire human\n",
      "genome, and the fine-tuning task could be classifying whether a particular sequence contains TATA promoters or not. The\n",
      "combination of pre-training and fine-tuning thus forms a powerful framework for learning from genomic data, balancing the\n",
      "benefits of unsupervised learning from large unlabelled datasets with supervised learning from smaller task-specific labelled\n",
      "datasets.\n",
      "Hyena\n",
      "Originally the Hyena layer type was introduced for NLP75, but more recently has been applied to genomic data in HyenaDNA49.\n",
      "The Hyena layer75was created to answer the demand for a transformer-like model to scale in terms of context length. The\n",
      "attention mechanism is bounded by quadratic cost in sequence length, and while adaptations have been made to improve the\n",
      "efficiency of the transformer47,48,79, novel architectures have been proposed to replace it entirely. These new model architectures\n",
      "must be able to be pre-trained like the transformer and exhibit an attention-like mechanism, without the cost of attention.\n",
      "Model types claiming to be the “next transformer” include the linear transformer (an adaptation of the transformer to calculate\n",
      "attention in linear-time, likening it to an RNN)80, RetNet (a linear transformer which removes the softmax in the attention\n",
      "calculation)81, and the Hyena layer75.\n",
      "The Hyena layer is so far the only one of these new model types to be applied to genomic data. The authors of Hyena75\n",
      "noted that there are two main properties of the attention mechanism that contribute to its success: global context, and data\n",
      "dependency. In attention, global context is achieved by having every token attend to every other token. The data dependency\n",
      "property of the attention mechanism is enforced by the softmax equation being applied to different projections of the inputted\n",
      "data. In contrast, a convolutional operator is not data dependent in this way, the same kernel is applied to every section of the\n",
      "inputted sequence. In attention, the input itself changes how the operation is applied.\n",
      "The Hyena authors proposed that the global context and data-dependency properties of attention could be satisfied by\n",
      "another approach, one that would have better scalability in terms of context length. To achieve this, the authors introduced a\n",
      "subquadratic drop-in replacement for attention, composed of implicitly parameterized long convolutions and data-controlled\n",
      "gating75. An implicitly parameterized convolution is the opposite of an explicit convolution, in which there are nparameters\n",
      "for the kernel size n. Instead, an implicit convolution is parameter-efficient. The parameters that define the convolution are\n",
      "learned by a Deep Feed-forward Neural Network (DFNN), such that there is a fixed parameter budget and any length kernel can\n",
      "be learned. A good parallel to this would be to learn the equation that defines a line ( y=mx+b), instead of learning every\n",
      "point that must be plotted on the line. While this ensures the Hyena layer is parameter-efficient, it does not explain how the\n",
      "layer is sub-quadratic in cost. Long convolutions cannot be naively applied, even implicit ones, as a convolution with a kernel\n",
      "size equal to the input length would result in a quadratic-cost computation like attention. To remedy this, the Hyena authors\n",
      "use the Fast Fourier Transform82to bring the cost of computing a long convolution down, essentially moving the convolution\n",
      "calculation from the time domain (with timesteps t) to the frequency domain, or the Fourier domain.\n",
      "Deep Learning for Genomics\n",
      "For a more general review on deep learning models applied to genomic data we suggest readers look into Li et al.57and Eraslan\n",
      "et al.58. Readers interested in the applications as well as progress and challenges of these tools should read Routhier et al.59\n",
      "and Spoval et al.60. For more background on model interpretation in genomics we direct readers to Novakovsky et al.55and\n",
      "Talukder et al.56. The main focus of this section will be to explore the transformer model and similar architectures (including\n",
      "HyenaDNA), and will discuss the previous architectures only as context for these model’s application in genomics.\n",
      "Before the Transformer\n",
      "CNNs\n",
      "CNNs offer a powerful tool for sequence analysis and the identification of complex regulatory patterns83. Using convolutional\n",
      "layers, CNNs exploit spatial hierarchies and the concept of weight sharing, enabling efficient detection of regulatory patterns\n",
      "and motifs regardless of their genomic location (within a certain kernel size) - a feature known as translational equivariance.\n",
      "The genomic sequence information is captured in convolutional layers by a series of operations with learned kernels that match\n",
      "specific DNA subsequences, demonstrating a versatile and adaptable method for processing genomic data55,63,84,85.\n",
      "Early CNN models for genomics, like DeepBind20, were simple architectures by today’s standards. Subsequent models, such\n",
      "as DeepSEA21and Basset33, increased model depth through additional convolutional layers. These models further expanded\n",
      "their context size, allowing them to process longer sequences and specialize in complex tasks like chromatin accessibility\n",
      "prediction. Later, the introduction of dilated convolutions, such as in the Basenji32model, marked a significant leap in the\n",
      "receptive field size of genomic CNNs. Dilated convolutions greatly expand a CNN model’s context size, enabling the capture of\n",
      "long-range dependencies without increasing a model’s computational complexity31,32,86.\n",
      "10/33\n",
      "Table 1. A summary of the recently proposed deep learning models covered in this review. In the architecture column the\n",
      "yellow colour indicates a transformer LLM model, the pink colour indicates a transformer-hybrid model, and the turquoise\n",
      "colour indicates a non-transformer LLM. The encoder-decoder column in colour-coded by encoder-only models, decoder-only\n",
      "models and encoder-decoder style models. The datasets trained on column has a red colouring for human-only models and blue\n",
      "for cross-species training. The interpretability column is coloured by presence or absence of model-interpretability analysis.\n",
      "11/33\n",
      "Many of the CNN-based models for genomics predict specific experimental assays given a DNA sequence. However, the\n",
      "Akita36model offers a unique perspective on DNA-sequence based prediction. Akita aims to model the three-dimensional (3D)\n",
      "structure of the genome by leveraging a CNN structure with dilated convolutions. Akita transforms one-dimensional (1D) DNA\n",
      "sequence data into two-dimensional (2D) representations indicative of genomic region contacts. The major innovation of the\n",
      "Akita model was the use of the dilated residual convolutions. Residual connections allow for increased context window size by\n",
      "optionally adding motifs captured from earlier layers back into later ones.\n",
      "The Orca model71, which succeeds the Akita in 3D genome prediction, implements an encoder-decoder framework using\n",
      "CNNs. The success of this approach motivates the use of classic encoder-decoder type frameworks in genomics.\n",
      "In single-cell transcriptomics modelling, the CNN has not been used widely, though it has been applied for de-noising data,\n",
      "by converting single-cell data into images first43,87.\n",
      "More recent CNN-based models for genomics have introduced modules beyond vanilla, dilated and dilated residual\n",
      "convolutions. The ExPecto25model includes a spatial transformation module to reduce the dimensionality of genomic\n",
      "predictions. Dimensionality reduction is a way to simplify data by reducing its number of features or “dimensions”, which\n",
      "can be done by removing redundancy or useless information. In ExPecto this is done through weighing different regions\n",
      "of the sequence captured by the CNN module based on their relative distance to a transcription start site (TSS). Sequences\n",
      "closer to the TSS are weighed as more important. This dimensionality reduction helps the ExPecto model by reducing the\n",
      "model’s likelihood to overfit (memorize the data) and lessening the computation time and storage requirements for the data.\n",
      "Additionally, the spatial transformation module allows for incorporation of spatial information from a larger context window,\n",
      "continuing with the general theme of genomic modelling88.\n",
      "The ExPecto model’s spatial transformation module provides a strong indication of the potential success of transformers\n",
      "applied to the same problem. The use of attention mechanisms in transformers allows for dynamically selecting subsequences\n",
      "as important from a larger region, rather than statically assigning importance based on sequence proximity to the TSS. Similarly,\n",
      "the Hyena75layer’s ability to take long convolutions over whole sequences with data-controlled gating expands sequence\n",
      "context while reducing the dimensionality of the sequence information.\n",
      "RNNs\n",
      "When RNNs were introduced to genomic sequence prediction, they were, for the most part, employed in combination with\n",
      "CNN modules29. As most RNNs have a “memory” feature, they are aptly suited for processing sequential data and capturing\n",
      "dependencies within a sequence18(see Supplemental Figure 4 (c)).\n",
      "An initial limitation of RNNs was the degradation of this “memory” over long sequences, which spurred the development\n",
      "of Long Short Term Memory networks (LSTMs). These networks introduced “smart” neurons that selectively “remember” or\n",
      "“forget” information, thereby overcoming the limitation of memory degradation inherent to some standard RNNs. Bi-directional\n",
      "LSTMs (BLSTMs) build on this technology, integrating dual-direction processing to incorporate information from both\n",
      "preceding and succeeding sequence elements. This is particularly relevant in genomics due to the lack of inherent directionality\n",
      "of the genome18(see Supplemental Figure 5 (b)).\n",
      "While the RNN enjoyed a lot of success in the NLP field, it was not broadly adopted for genomic modelling24,29,43,89.\n",
      "Perhaps this was due to the imminent introduction of the transformer model, which used attention to replace recurrence,\n",
      "allowing for parallelized computations on GPU hardware that made the model easier to scale. Interestingly, several recent\n",
      "adaptations on the transformer block aiming to increase context-size have re-introduced recurrence to their architecture, as the\n",
      "attention mechanism is becoming a bottleneck for compute and memory75,80,81.\n",
      "The Transformer\n",
      "For genomic modeling, transformers are employed in two primary ways: either as a subsequent module following initial layers,\n",
      "or as a standalone transformer model.\n",
      "In the former, a set of initial layers precede the transformer modules, these layers compress broad contextual data into\n",
      "a lower-dimensional embedding space. This approach is often necessitated by the quadratic computational cost of attention\n",
      "mechanisms.\n",
      "When an extended context window is not a requirement for downstream tasks, transformers can operate as standalone models.\n",
      "Inputs are transformed using a limited number of distinct tokens and processed directly by the transformer module. Models that\n",
      "adopt this approach are primarily aimed at creating Large Language Models (LLMs) tailored for genomic sequences.\n",
      "Therefore, we split the transformer-based models for genomics into two classes: hybrids and LLMs. Hybrid models, which\n",
      "incorporate transformers as one element within a more intricate architecture, may not meet the criteria for being considered true\n",
      "LLMs. Unlike LLMs, which are designed for comprehensive understanding and generation of language-like sequences, hybrid\n",
      "models are specialized for tasks such as predicting experimental assays like Cap Analysis Gene Expression (CAGE) tracks and\n",
      "ChIP-seq. These models pick up where the traditional genomic models, such as DeepBind, left off. They specialize in higher\n",
      "accuracy performance on many of the same tasks as their CNN predecessors20,21. Hybrid models often forgo pre-training, a\n",
      "12/33\n",
      "hallmark feature commonly associated with LLMs. This lack of pre-training further distinguishes them from LLMs, which are\n",
      "generally more versatile and generalizable.\n",
      "Hybrid Transformers: Assay Prediction\n",
      "SATORI (Self-ATtentiOn based model to detect Regulatory element Interactions) is a transformer-based model. The model\n",
      "aims to capture a global view of the interactions between regulatory elements in a sequence to infer cooperativity. This model\n",
      "uses a convolutional layer in addition to a self-attention mechanism (see Figure 2). The convolutional layer is used to discover\n",
      "motifs in the input sequences, while the self-attention layer is used to capture potential interactions between these features.\n",
      "This approach allows the model to infer a global landscape of interactions in a given genomic dataset directly, rather than as a\n",
      "post-processing step.\n",
      "In SATORI, the sparsity inherent to the attention matrix allows it to be used as a proxy for covariance. Additionally, the\n",
      "model was proposed as “interpretable” due to direct analysis of the attention matrix. However, treating the attention mechanism\n",
      "within transformers as directly interpretable has drawbacks90.\n",
      "Within a few months of SATORI, the Enformer model was published. Enformer uses transformers to predict various\n",
      "genomic track signals including gene expression, DNA accessibility, and histone modification or TF-binding information31. The\n",
      "convolutional blocks and transformer blocks within Enformer work together to dynamically summarize and capture long-range\n",
      "dependencies in DNA sequence. The model’s final layers bifurcate into organism-specific network heads for prediction, having\n",
      "been trained on both human and mouse data.\n",
      "Enformer represents an evolution in the prediction of quantitative genomic assays. The model moves away from the\n",
      "traditionally utilized convolution-based architectures such as Basenji32and ExPecto25. Although convolutional models have\n",
      "shown efficacy in capturing DNA sequence patterns, they, like the transformer, can also suffer from a limited context window.\n",
      "Naive long-convolutions are as computationally intensive as attention when the filter size of the convolution is equivalent to\n",
      "the sequence length. While dilated convolutions can mitigate this by introducing ‘holes’ in the filter, there is still a limit to\n",
      "the expansion of the CNN context size. Enformer’s innovation is balancing the use of convolutions, dilated convolutions, and\n",
      "transformers to maximize context length while also capturing much of the local sequence information.\n",
      "Recent work by Karollus et al.91scrutinized models like Enformer and Basenji286, emphasizing the need for more and\n",
      "better-curated training data. They noted that despite Enformer’s dramatically increased context window, it still encountered\n",
      "considerable limitations in predicting the impact of distal regulatory elements, such as enhancers. Enformer’s predictive power\n",
      "remains comparably robust even with a severely restricted input window, suggesting its receptive field size is not the sole\n",
      "determinant of its success. This success can instead be attributed to innovations in model architecture such as combinations of\n",
      "various layer types, overall parameter number, or the quantity of data it was trained on. Karollus et al.91propose that in order\n",
      "for models to accurately account for distal regulators’ contributions they must train on datasets curated with an emphasis on\n",
      "long-range signals.\n",
      "Borzoi62builds on the Enformer architecture by doubling the size of the context window. Borzoi further expands the number\n",
      "of experimental assay predictions compared to Enformer, and introduces predictions of RNA-seq coverage. The model’s main\n",
      "innovation is using an architecture styled after U-Net to upsample and increase resolution prediction following the transformer\n",
      "module. The convolutional blocks preceding the transformer summarize the longer sequence input and transform it into the\n",
      "same resolution as in Enformer. This allows the attention calculation to be calculated with similar compute, thereby avoiding\n",
      "the quadratic memory complexity by increasing sequence resolution. To make final predictions at a higher resolution, the\n",
      "information captured by the transformer is expanded to a larger sequence length using convolutions.\n",
      "C.Origami is the hybrid transformer iteration on 3D genome prediction36,45,71. The model makes de novo predictions of\n",
      "cell type-specific chromatin architecture from sequence. C.Origami incorporates both DNA sequence and genomic signals\n",
      "(CTCF-binding and ATAC–seq) to do this. Like its predecessor, Orca71, C.Origami uses an encoder-decoder design, with the\n",
      "modification of using two encoders to deal with multi-modal input types (one for DNA sequence and one for genomic signals).\n",
      "The novelty of this design also includes following the dual-encoders with a transformer module before the task-specific decoder.\n",
      "After the 1D-CNN encoders, genomic feature representations from both encoders are concatenated and fed into the transformer\n",
      "module to enable long-range information exchange. This, along with the addition of genomic features, allows C.Origami to\n",
      "outperform Orca45,71, along with Akita36. The C.Origami model enables in silico experiments that examine the impact of\n",
      "genetic perturbations on chromatin interactions, and moreover, leads to the identification of a compendium of putative cell\n",
      "type-specific regulators of 3D chromatin architecture.\n",
      "Transformers: Large Language Models\n",
      "One of the earliest Large Language Models (LLM) for the genome also happens to be one of the earliest uses of a transformer\n",
      "applied to the genome. DNABERT was adapted for genomic sequence modelling30based on the original BERT model3.\n",
      "DNABERT is trained using a method similar to BERT3, with some modifications to suit the genomic context. The model\n",
      "is pre-trained on overlapping k-mers of genomic sequence using unsupervised learning with the MLM pretext task. This\n",
      "13/33\n",
      "pre-training phase allows the model to learn a rich representation of the genomic sequence data, capturing both local and\n",
      "long-range dependencies.\n",
      "After pre-training, DNABERT is fine-tuned on specific tasks, such as predicting the presence of transcription factor binding\n",
      "sites or other regulatory elements. This involves training the model on a smaller amount of labeled data, with the objective\n",
      "of predicting the labels based on the input sequence. The fine-tuning phase allows the model to adapt the representations\n",
      "learned during pre-training to the specific task at hand. The DNABERT model, as an LLM, does not directly predict regulatory\n",
      "annotation measurements. Instead, DNABERT predicts the absence or presence of binding sites, promoters, etc. given a specific\n",
      "classification objective it is fine-tuned on.\n",
      "DNABERT has state-of-the-art performance in predicting proximal and core promoter regions (two specific kinds of\n",
      "promoter types), and presence of transcription factor binding sites. However, the model has a more limited context window\n",
      "relative even to previous CNN and RNN-based models. Like the original BERT, the model cannot take in more than 512 tokens,\n",
      "or in this case, k-mers. To mediate this limitation, the authors designed DNABERT-XL, a version of DNABERT where the\n",
      "authors split longer sequences into multiple pieces, each under 512 base pairs in length, and independently feed these pieces to\n",
      "DNABERT. They then concatenate the final representations together and feed the concatenated representations to the output\n",
      "layer. While this approach to increasing context-window size was able to identify between TATA and non-TATA promoters\n",
      "well, they did not demonstrate an end-to-end approach in modeling complex long-range dependencies, due to the limitations of\n",
      "the cost of attention.\n",
      "The major advantage of DNABERT lies in its capacity for self-supervised pre-training, a product of its transformer\n",
      "architecture. As seen in Table 1, the model is pre-trained extensively before any fine-tuning tasks. This alleviates the need\n",
      "for large amounts of labelled task-specific data, or extensive training time and compute spent when fine-tuning the model for\n",
      "specific downstream tasks.\n",
      "DNABERT’s k-merization scheme for tokenizing the genome was adopted by many transformer-based LLMs for the\n",
      "genome, including the Nucleotide Transformer46. The Nucleotide Transformer consists of a family of transformer models with\n",
      "varying training regimes and parameter sizes. These models include a 500 million parameter model trained on the human\n",
      "genome, a 500 million and a 2.5 billion parameter model trained on a large set of genetically diverse human genomes, and a\n",
      "2.5 billion parameter model trained on genomes from hundreds of species. Each of the Nucleotide Transformer models were\n",
      "trained on hexamer versions of their genomic datasets and pre-trained using the MLM pretext task, like DNABERT. However,\n",
      "the Nucleotide Transformer models replaced the overlapping k-merization of DNABERT with a non-overlapping version that\n",
      "greatly reduced tokenized sequence length. The replacement of overlapping k-mers with non-overlapping ones also addressed a\n",
      "major limitation of overlapping k-mers in MLM tasks, where information from a masked token is leaked by adjacent tokens39.\n",
      "However, the non-overlapping k-mer approach comes with its own limitations, primarily that the insertion or deletion of a single\n",
      "nucleotide base leads to dramatic changes in tokenized sequences39. This dramatic change in representation of sequences,\n",
      "despite their similarity, prevents the model from easily aligning proximal sequences within the token space, which can impede\n",
      "the ease of training.\n",
      "The smallest of the Nucleotide Transformer models is five times larger than DNABERT, and the authors’ benchmarking\n",
      "experiments (predicting enhancers, promoters, TATA promoters, splice sites etc.) show that increasing model size yields better\n",
      "performance. This is in line with the same intuition that has led assay-prediction models like Enformer and its predecessors to\n",
      "continue increasing their parameter sizes. The results of the Nucleotide Transformer models’ benchmarking experiments also\n",
      "showed that training with intra-species variability (using multiple genomes of a single species, such as thousands of human\n",
      "genomes) did not perform as well as training with inter-species variability (their multi-species training regime). This could be\n",
      "due to the multi-species models better capturing functional importance conserved across evolution, allowing them to generalize\n",
      "better even on human-based prediction tasks. The Nucleotide Transformer models strongly suggest that models that leverage\n",
      "evolutionarily diverse data in pre-training will have improved capacity in capturing functional relevance.\n",
      "The next iteration of DNABERT, DNABERT-239(released by the same group as DNABERT), followed a multi-species\n",
      "training regime. DNABERT-2 also utilized an alternative tokenization scheme, replacing k-mer tokenization entirely, instead\n",
      "favouring a compression algorithm used more widely by LLMs in NLP known as Byte Pair Encoding (BPE)92. This approach\n",
      "bypasses the issues associated with overlapping tokenization (leakage), and non-overlapping tokenization (unnecessary distance\n",
      "in the token space between similar sequences). The BPE approach iteratively merges frequent pairs of nucleotides or segments\n",
      "within the genome, as opposed to using a specific k-mer. This results in the model’s vocabulary comprising a set of variable-\n",
      "length tokens that together represent the entire genome dataset across species. BPE tokenization of DNA sequences has been\n",
      "observed to result in biologically significant tokens, with the longest tokens corresponding to elements of the genome known to\n",
      "be repetitive93. This is in contrast with the arbitrary tokens utilized when k-mer tokenization is used. Furthermore, the BPE\n",
      "method remains as computationally efficient as non-overlapping tokenization.\n",
      "Along with the introduction of BPE instead of overlapping k-merization, the DNABERT-2 authors employ several other\n",
      "methods to improve computational efficiency over DNABERT, including the use of Flash Attention79, among others94,95. Flash\n",
      "14/33\n",
      "Attention79is an IO efficient manipulation of the attention mechanism, optimizing for read and write time in memory as well\n",
      "as improving computational efficiency by splitting the keys, queries and values into blocks that have softmax incrementally\n",
      "performed over the entire input. These modifications allow DNABERT-2 to perform comparably to the Nucleotide Transformer\n",
      "models in several tasks, despite 21 times less parameters and significantly less computational cost.\n",
      "Another recently introduced family of transformer-based DNA LLMs is GENA-LM93. Like DNABERT-2 it uses BPE\n",
      "tokenization, and like Nucleotide Transformer there are both human-only and multi-species models with varying amounts\n",
      "of parameters. However, a significant difference between GENA-LM and other models is the use of sparse attention to help\n",
      "mitigate the quadratic complexity in context length of the transformer’s attention mechanism. Sparse attention allows for\n",
      "modelling distant dependencies within the sequence, but does not perform full pairwise attention between all tokens. This\n",
      "results in GENA-LM models having increased maximum sequence length over other transformer-based LLMs for the genome,\n",
      "with a maximum tokenized sequence length of 4096 tokens. The median token length after BPE tokenization is nine base pairs,\n",
      "thus GENA-LM models can process sequences of up to 36 thousand base pairs.\n",
      "Transformers: Non-sequential Genome LLMs\n",
      "Within the family of transformer-based LLMs for the genome, there are two recently proposed models that have taken a\n",
      "radically different approach to modelling the genome. Specifically, these models depart from modelling DNA or RNA in a\n",
      "sequence-based manner, instead training on non-sequential single-cell omic data such as cell profiles and gene expression\n",
      "counts.\n",
      "The first of these models is Geneformer41, which operates on non-sequential single-cell omic data. Geneformer represents\n",
      "cell embeddings with only gene expression “rankings”, ignoring precise gene-expression measurements and instead treating\n",
      "single-cell omic data as qualitative. Geneformer was trained on a 30 million cell corpus, consisting of only healthy human cells.\n",
      "The input to Geneformer consists of a rank value encoding, such that genes are inputted in order of their expression ranking\n",
      "within the cell (normalized across the entire single-cell corpus). Geneformer’s input has a length limited to 2,048 tokens, still\n",
      "making it the largest input with full dense self-attention for transformer-only models in genomics so far. These 2,048 input\n",
      "tokens are the rank encodings of all genes within a given cell that have non-zero expression. The attention matrix is used to\n",
      "learn gene-to-gene relationships within the context of each cell, over the full 2,048 gene input.\n",
      "This approach to reporting the presence or absence of genes within a cell, and ranking the present genes within the\n",
      "embedding, benefits from observing gene expression values for a single cell many times across the 30 million cell corpus. The\n",
      "authors state this approach encourages highly expressed housekeeping genes to be normalized to lower ranks, while genes that\n",
      "identify cell state with high fidelity, but which may be expressed at low levels, move to higher rankings. The ranking approach\n",
      "may even have benefits over reporting gene expression values quantitatively by offering a buffer against technical artefacts that\n",
      "bias these counts.\n",
      "The Geneformer model reports excellent performance in gene dosage sensitivity predictions, chromatin dynamics predictions,\n",
      "and network dynamics predictions.\n",
      "Like Geneformer, the recently proposed single-cell Generative Pretrained Transformer model, or scGPT40, takes as input\n",
      "non-sequential genome data. However, unlike Geneformer, scGPT uses a unique tokenization scheme to handle single-cell\n",
      "transcriptomic data and marks one of the first uses of generative pre-training for transformers applied to genomics. The input\n",
      "to scGPT is described analogously to word and sentence embeddings used in generative NLP where cells (sentences) are\n",
      "characterized by genes and the protein products they encode (words). Each input embedding of length 1,200 to scGPT therefore\n",
      "represents a cell composed of gene IDs, their binned expression values (which converts all expression counts into relative\n",
      "values), and condition tokens representing additional learned features or meta-information. Including metadata tokens allows\n",
      "scGPT to accept as input multiple single-cell omic data modalities, or single-cell multi-omic data, including scATAC-seq data.\n",
      "Prior to scGPT’s approach there was no concrete way to aggregate multiple single-cell data modalities in one foundation model\n",
      "such that biological signal was preserved and experimental noise was reduced.\n",
      "Along with scGPT’s novel tokenization scheme, the author’s introduced a novel attention mask for the generative pre-\n",
      "training stage. scGPT’s generative pre-training was based on the GPT6,8generative pre-training, where models aim to predict a\n",
      "next most likely token based on a “prompt” of known inputs. The motivation for using a generative approach with “prompting”\n",
      "to model single-cell omic data, beyond zero-shot generalization advantages, was twofold. scGPT was designed to solve both\n",
      "the problem of generating unknown gene expression values given known gene expression values (“gene prompts”), as well\n",
      "as generating whole genome expression given a known input cell type condition (“cell prompts”). In order to achieve this,\n",
      "scGPT required a unique attention mask capable of supporting gene-prompt and cell-prompt generations. This was achieved by\n",
      "only supporting attention computations between embeddings of “known” genes and the query gene itself. As the generation\n",
      "iterations continue, the predicted gene expression values for the new set of query genes become the “known” genes for the next\n",
      "iteration. This forces sequential predictions in non-sequential single-cell omic data, with parallels to causal attention masking\n",
      "in vision.\n",
      "In contrast, scGPT’s fine-tuning training regime employed a non-generative approach, instead using MLM for optimal\n",
      "15/33\n",
      "performance. scGPT, pre-trained on 33 million healthy human cells, achieves state-of-the-art performance in genetic perturbation\n",
      "prediction, batch correction, and multi-omic integration, further demonstrating the power of “pre-training universally, fine-tuning\n",
      "on demand”.\n",
      "Beyond the Transformer\n",
      "While the transformer model is often considered synonymous with Large Language Models, especially with the recent success\n",
      "of ChatGPT6,8, LLMs are not always transformer-based. There are other architectures that can be trained to perform as well\n",
      "as LLMs, and can even undergo similar pre-training regimes including leveraging MLM or ALM pretexts74,75. As of now, it\n",
      "remains unclear whether the success of the transformer model lies in an artefact of the architecture, like the attention mechanism,\n",
      "or whether this mechanism simply allowed these models to scale up more quickly than their counterparts. It could be that the\n",
      "pre-training capabilities of the transformer, which are not restricted to this architecture, contribute immensely to its success. If\n",
      "this is the case, the transformer has the potential to be overtaken in NLP, and genomics as well75,80,81.\n",
      "The Genomic Pre-trained Network, or GPN77, copied the exact architecture of a transformer encoder module, but replaced\n",
      "the attention mechanism with a convolution operation across the sequence. The idea behind this came from recent work that\n",
      "showed pre-trained CNNs are competitive with transformers in NLP74, and protein modelling96. The GPN model leverages the\n",
      "MLM pretext task in pre-training, and is trained solely on individual nucleotides, rather than using BPE or any k-merization\n",
      "strategy. The genomes used in pre-training consisted of eight Brassicales reference genome assemblies from NCBI Genome.\n",
      "Instead of sampling the whole genome equally in 512 base pairs windows during pre-training, the authors took the union\n",
      "of exons (with a small intronic flank), promoters (1,000 base pairs upstream of the TSS) and a complimentary number of\n",
      "random windows from the whole genome. While the authors state this may have improved performance, they do not show any\n",
      "experiments to validate this claim.\n",
      "The authors demonstrate GPN learns non-coding variant effects from unsupervised pre-training on genomic DNA sequence\n",
      "alone, outperforming supervised deep learning models such as DeepSEA.\n",
      "Another non-transformer genome LLM, HyenaDNA49, achieves a context size of 1 million nucleotides, 500x larger than\n",
      "the largest of the foundational models utilizing full pairwise attention, the Nucleotide Transformer46. Instead of relying on the\n",
      "quadratic-bound attention mechanism, which compares each pair of points in a sequence, the authors of the original Hyena\n",
      "paper designed a subquadratic-time layer. This layer is constructed by interleaving implicitly parameterized long convolutions\n",
      "and data-controlled gating. The Hyena layer incorporates several artefacts from CNNs, RNNs and transformers. The long\n",
      "convolution is essentially a convolutional layer performed with a filter/kernel size equal to the sequence length. The long\n",
      "convolution is implicitly parameterized, meaning the filter is represented as a parametric function of the time step. This allows\n",
      "the model to capture dependencies across the entire input sequence without a prohibitive increase in computational cost or risk\n",
      "of overfitting. The data-controlled gating works similarly to the ability of LSTMs to “forget” certain information, learning a\n",
      "function that computes the gating values based on the input data. From the transformer, the Hyena layers have borrowed the\n",
      "idea of global interaction mapping, and taken the famous pre-training regime associated with transformers, but swapped the\n",
      "quadratic complexity of attention for long convolutions and data-controlled gating. HyenaDNA is based off of the structure of\n",
      "the decoder-only transformer architecture, replacing the attention mechanism directly with the Hyena operator. HyenaDNA\n",
      "is trained generatively using the ALM pretext task, like scGPT40. The HyenaDNA model was only trained on one reference\n",
      "human genome, providing an obvious direction for future work. All in all, the model boasts state-of-the-art performance on all\n",
      "eight datasets from GenomicBenchmarks97.\n",
      "Limitations\n",
      "As previous review papers have focused on the limitations inherent in applying deep learning models to genomic data, including\n",
      "cell-type specificity debates and training data limitations58, we focus on the limitations inherent to the application of novel\n",
      "architectures to genomics, such as the transformer and Hyena.\n",
      "Long Range Interactions\n",
      "Transformer-hybrid models, despite high accuracy on specific tasks for which large amounts of labelled training data exist,\n",
      "still appear unable to capture long-range dependencies within the genome. Most improvements in assay prediction models\n",
      "are aimed at increasing context window in an effort to better model these long-range dependencies. However, Karollus et\n",
      "al.91suggest a larger receptive-field might not be the key factor driving success in more recent models, including Enformer31.\n",
      "While trends show models with increased context window size have had higher accuracy on predicting experimental assays,\n",
      "this does not necessarily suggest context window size is the driving force behind improved predictive capacity. Karollus et\n",
      "al.showed that significantly reducing the input size given to the Enformer model has a minimal effect on its performance,\n",
      "suggesting that transformer-based models, like Enformer, may achieve state-of-the-art performance simply due to the addition\n",
      "of the transformer module, or through increased parameter size. Enformer’s successor, Borzoi62appears to better integrate\n",
      "16/33\n",
      "Figure 3. The total amount of compute, in petaflop/s-days (PFS-Days) used to train the various models discussed in the\n",
      "review (all of the models for which parameter number, training time, and GPU usage was available). A PFS-Day consists\n",
      "of performing 1015 neural net operations per second for one day, or a total of about 1020 operations. It is a compute-time\n",
      "measurement proposed by OpenAI to compare across model architectures which can be thought of similarly to kW-hr for\n",
      "energy. For context, we calculate the PFS-Days for the original DeepSEA model as well as the equivalent PFS-Days that can be\n",
      "purchased to train a model with $5,000 USD, renting 8 A100 GPUs at $8.80/hour. Calculations for PFS-Days can be found in\n",
      "the Limitations section.\n",
      "17/33\n",
      "long-distance information, as measured by ranking distal regulatory elements for their gene-specific enhancer activity (data from\n",
      "CRISPR screens). However, there have not yet been experiments done to evaluate Borzoi’s62performance after a significant\n",
      "reduction of the input size, similar to those performed on Enformer31. Likely, context window size has to be combined with\n",
      "better-curated datasets, ones that are curated to capture the effects of distal eQTLs, distal enhancers and distal repressors.\n",
      "If such a dataset can be curated, then context-window size will be the driving factor for modelling long-range dependencies.\n",
      "In that case, transformer-based LLMs, without being preceded by downsampling techniques to reduce dimensionality prior\n",
      "to the attention calculation, or applying a more efficiently implemented attention method79, will be limited by their context\n",
      "window. Even the Nucleotide Transformer46, with 2.5 billion parameters, was only able to extend the context window to a max\n",
      "of 1,000 tokens, which still remains 1,000 times smaller than the HyenaDNA49context window.\n",
      "Other-LLM models that forgo the use of the quadratic attention mechanism have potential to better capture long-range\n",
      "interactions in the genome. As long as the attention mechanism itself is not the driving force of success in these models, which\n",
      "does not appear to be the case given the recent success of HyenaDNA49, this is a strong avenue for potential research.\n",
      "Cell type Specificity\n",
      "Transformer-hybrid models, which aim to predict experimental assays, are usually trained with either ENCODE98data from\n",
      "multiple cell lines, or fine-tuned non-specifically across cell types. The tasks they are evaluated on make predictions ignoring\n",
      "the inter-cell type variability that exists within genomic annotations. While this allows the models to leverage huge amounts of\n",
      "data available on ENCODE98and Roadmap99by pooling cell types together, many findings in the field of genomics and the\n",
      "increased used of single-cell specific sequencing suggests there is cell-specific heterogeneity in regulatory annotations like\n",
      "chromatin accesssibility, chromatin conformation, gene expression, and transcription factor binding26,100,101. Some transformer\n",
      "models have recently been proposed to mitigate the bias of cross-cell lines gene expression prediction using transfer learning,\n",
      "but this approach has yet to be commonly adopted102. A future area of research could be moving away from transformer-hybrid\n",
      "models for predicting cell-type specific experimental assays. Instead, the prompting capabilities of generative LLMs could be\n",
      "leveraged to create cell-type specific contexts for predictions.\n",
      "Data Privacy\n",
      "As mentioned, the majority of the models discussed in this review are trained on public datasets like ENCODE98and Roadmap99.\n",
      "However, there is a risk of re-identification of individuals even from anonymized genomic datasets due to the uniqueness of\n",
      "genetic information. As more data continues to be integrated into these models, and these models are potentially leveraged for\n",
      "use on private datasets including in clinical settings, there is an increasing need for developing and implementing more robust\n",
      "de-identification techniques and privacy-preserving algorithms, such as differential privacy103and federated learning104.\n",
      "Interpretability\n",
      "Beyond limitations in capturing long-range dependencies, a fundamental limitation of applying deep learning models to\n",
      "genomic data lies in their black-box nature. As these models become larger and more complex, it becomes less obvious how to\n",
      "explain and understand the decisions they make, especially in the context of genomics where the underlying “language” of the\n",
      "genome is as of yet unclear to us56. Previous papers have explored the interpretability of deep learning models in the context of\n",
      "genomics extensively18,55,58, here we focus specifically on the task of interpreting transformer models and similar architectures.\n",
      "Attention scores of transformers have been proposed as an interpretability solution to the problem of black box models\n",
      "in genomics30,31,105. This had led to models like Enformer31, DNABERT30and C.Origami45, and more, directly reporting\n",
      "analysis of attention scores from their models as proof of their ability to capture biological signal. However, several studies on\n",
      "transformer models outside the context of genomics have shown attention is not inherently interpretable106,107. Specifically,\n",
      "reporting raw attention scores alone reduces the information captured by the model to only the attention scores, ignoring most\n",
      "of the attention components by only accounting for the inner products of queries and keys, rather than the entire computation\n",
      "of queries, keys and values. Furthermore, the attention scores taken from a specific layer in the model ignore other layers\n",
      "involved in the model’s decision-making process. Aggregation across multiple layers in a naive summation or mean does not\n",
      "account for the way attention scores move through the model, through add-and-norm layers and skip-connections44,107. Models\n",
      "that consider the mean of attention heads across multi-headed attention also dilute the information captured by the model as\n",
      "different heads contribute differently in each layer, and not all heads contribute equally106–108.\n",
      "Recently, other methods like attention flow and attention rollout109, along with Layer-Wise Relevance Propagation\n",
      "(LRP)107,108have been applied to interpret transformer models with success.\n",
      "The attention rollout method makes the assumption that input tokens are linearly combined through the layers of a\n",
      "transformer according to their attention weights. The weights are then rolled out to capture the propagation of information from\n",
      "input tokens to hidden embeddings in a given layer i, recursively multiplying the attention weights matrices. Attention flow\n",
      "treats the problem of mapping the contribution of input token attention weights from final layers as a maximum flow problem.\n",
      "In other words, the attention weights are treated as the capacity of each connection between neurons, where computing the\n",
      "18/33\n",
      "attention in a layer iback to the inputs is treated as finding the maximum flow value from each input token to each position\n",
      "in layer i. Both methods, attention rollout and attention flow, allow for quantification of the flow of information through\n",
      "self-attention in a transformer. This allows the attention weights within the model to be correctly mapped back to the input\n",
      "tokens and given a measure of their relative relevance to model outputs. These methods are highly correlated with one another\n",
      "and have been shown to outperform raw attention scores alone in determining token contributions to outputs109. However, the\n",
      "rollout approach is limited by treating input contributions as neutral rather than distinguishing between positive and negative\n",
      "contributions of tokens to final decisions. Additionally, the attention flow method is computationally expensive.\n",
      "LRP is a method which has been utilized to understand and interpret the decisions made by deep learning models like\n",
      "CNNs90. LRP works by attributing the contribution of each input feature to the final decision of the network. This is achieved\n",
      "by propagating the output prediction back to the input layer, thereby providing an indication of feature importance.\n",
      "LRP is calculated as110:\n",
      "Rj=∑\n",
      "kajwjk\n",
      "∑\n",
      "0,jajwjkRk (1)\n",
      "Where jandkare the neurons of consecutive layers, ais the activation of the respective neuron, and wis the weight\n",
      "between two neurons. Rkis then the ‘relevance’ received by neuron k, which is interpreted as the contribution of that neuron in\n",
      "its layer to the output prediction f(x).\n",
      "LRP has been widely used in interpreting CNN-based models, providing valuable insights into their decision-making\n",
      "process90. LRP has also been applied to transformers, particularly to demonstrate multi-headed attention results in “redundant”\n",
      "heads108. A recent paper has further adapted the LRP method for explaining transformer classification decisions, incorporating\n",
      "information from attention scores across multiple heads and re-weighting contributions by relevance107. This method has\n",
      "outperformed other attribution methods like classic LRP111, partial LRP108, rollout109and Grad-cam112on transformers.\n",
      "Beyond adapting interpretability methods for transformers, model-agnostic methods like SHAP (SHapley Additive\n",
      "exPlanations)113, and more recently, weightedSHAP114, provide obvious avenues of exploration. SHAP quantifies the\n",
      "contribution of each feature to a model’s prediction on a specific data point. WeightedSHAP extends this by considering the\n",
      "importance of each data point, useful when some instances matter more than others.\n",
      "Ultimately, interpretation of transformers in genomics beyond attention-score visualization has been limited. We acknowl-\n",
      "edge that while previous papers provide some insight into transformers’ interpretability specifically in genomics105, they have\n",
      "not compared methods beyond attention scores or acknowledged the limitations of this method106.\n",
      "Outside of the transformer models covered in this review, the other LLM models must be evaluated for their interpretability\n",
      "in the context of genomic tasks. GPN77reported motifs captured in the convolutional blocks of the architecture as a metric\n",
      "of interpretability, similar to previous CNN-based models for genomics20,21,23. These motifs were then compared to experi-\n",
      "mentally determined and validated motifs and reported as Position Weight Matrices (PWMs) or logos. Interestingly, despite\n",
      "HyenaDNA’s49success in genomic modelling, the authors did not dedicate a section of their paper to model interpretability.\n",
      "This is likely due to the novelty of the Hyena layer, as interpretability methods have not yet been explored for this architecture.\n",
      "Potential routes to address this issue could involve examining the motifs captured by the model and used for the “gating”\n",
      "mechanism within the Hyena layer’s “memory”.\n",
      "The transformer-based models covered in this review have reported attention scores for their interpretability metrics, with\n",
      "the SATORI115authors even claiming the model was inherently interpretable due to the attention mechanism alone. While these\n",
      "models have not employed more sophisticated methods for model interpretability, attention mechanisms appear to encourage\n",
      "exploration of model interpretability. In contrast, only one of the non-transformer LLM models (GPN77) reported some kind of\n",
      "model interpretability. We suggest researchers in this area who want to employ the use of transformer-based models analyze\n",
      "their models beyond raw attention scores alone, by incorporating methods like classic LRP111, partial LRP108, SHAP113or\n",
      "weightedSHAP values114and rollout109to discover biological motifs and tokens of interest these models attend to. This will\n",
      "potentially result in richer and more nuanced explanations of genomic models that use attention mechanisms, and could lead to\n",
      "more meaningful biological insight. Furthermore, we encourage the concurrent development of interpretability methods for the\n",
      "novel architectures proposed to this field beyond transformers, like HyenaDNA49.\n",
      "We expect the use of interpretability methods for attention-based mechanisms in genomics to increase, given their increasing\n",
      "usage trend among transformers applied to other fields107. Similarly, we expect models that aim to unseat the transformer in\n",
      "genomics invest in interpretability methods along with novel architecture design.\n",
      "Compute Requirements\n",
      "Perhaps the most noteworthy limitation in the usage of LLMs for the genome, irrespective of the use of transformer, convolution,\n",
      "or Hyena layer, is the computational cost of training these models. Their training regimes, pre-training in particular, often\n",
      "necessitate high-performance computing resources, which may not be readily accessible or affordable for all research teams.\n",
      "19/33\n",
      "Using a compute calculation derived by OpenAI, we can calculate the peta-flop(s)/days for each of the models discussed in\n",
      "the review, with the exception of GENA-LM93for which there was insufficient data on days trained, in order to compare across\n",
      "different model architectures and parameter numbers. The equation for PFS-Days using GPU Time is:\n",
      "PFS-Days =Number of GPUs ×(peta-flops /Hardware )×Days Trained ×Estimated Utilization\n",
      "Where OpenAI assumes a 33% utilization for GPUs. Looking at Figure 3 we can see that the majority of models proposed\n",
      "using transformers or similar architectures for genomics can be trained with $5,000 USD on 8 A100 GPUs. However, of the\n",
      "largest and arguably most discussed models (DNABERT30, Enformer31, Nucleotide Transformer46and HyenaDNA49) only\n",
      "DNABERT30can be trained with $5,000 USD worth of PFS-Days. However, DNABERT was trained prior to 2021, when GPU\n",
      "access was more restricted and more expensive than it is today, meaning the model likely could not have been trained using the\n",
      "equivalent of $5,000 PFS-Days at that time.\n",
      "The transformer and transformer-based models reviewed in this paper necessitate significant processing power and memory\n",
      "due to their complex architecture, which includes multiple layers of multi-headed attention mechanisms. Even the Hyena\n",
      "model, which boasts much better time and memory complexity than the attention mechanism, requires more compute than most\n",
      "academic labs can afford (setting $5,000 USD as a baseline). Additionally, even with compute available, transformer-based\n",
      "models and HyenaDNA as well, can take a considerable amount of time to train, see Figure 2. This can slow down the research\n",
      "process and make it more difficult to iterate and improve models.\n",
      "For pre-training tasks, even when leveraging unsupervised learning, these large models typically require large amounts of\n",
      "training data to perform optimally. In the context of genomics, this could mean needing access to extensive datasets of genomic\n",
      "sequences, of which there is not currently a lot of cell type specific data. While unsupervised pre-training can mitigate the need\n",
      "for labelled data, many of these models require fine-tuning to perform well on downstream tasks, and due to the size of the\n",
      "models, still require significant amounts of labelled data to fine-tune effectively.\n",
      "Pre-training Task Design\n",
      "Pre-training is a powerful and architecture-agnostic tool for genome LLMs, but its effectiveness hinges on the quality of the\n",
      "task assigned for pre-training. At best, pre-training allows deep learning models to capture universal data patterns116, at worst,\n",
      "it is an unnecessary expenditure of compute117. To maximize the benefits of pre-training for a genomic LLM, the initial task\n",
      "should be both biologically relevant and useful for later applications.\n",
      "Consider a genome LLM, G, fine-tuned to predict disease outcomes from DNA sequences, G(X) =Y. Using common\n",
      "pre-training tasks like ALM may impose flawed assumptions, such as treating DNA sequences as unidirectional. While MLM\n",
      "accounts for DNA’s bi-directional nature, it’s not without its own limitations.\n",
      "Often, pre-training tasks from other fields like natural language processing (NLP) are applied to genomics118without\n",
      "consideration of their biological relevance and the inherit differences between the genome and natural language. These tasks may\n",
      "not capture meaningful biological signals or align with the model’s ultimate goal. A better approach is to design a pre-training\n",
      "task based on biological insights. For example, pre-training Gto estimate the evolutionary viability of a mutated sequence\n",
      "would be more aligned and scientifically sound. One recently proposed technique for biologically informed unsupervised\n",
      "pre-training of a deep learning model on genomic data is Self-GenomeNet118. Self-GenomeNet exploits the structure of\n",
      "genomic data by utilizing the reverse-complement of sequences during training, with the pretext task being to predict the\n",
      "embedding of the reverse-complement of the sequence which follows a given sequence. Thus, the model learns to embed in\n",
      "a representation of a given sequence the information necessary to reconstruct the reverse-complement of the neighbouring\n",
      "sequence. Self-GenomeNet has been demonstrated to perform well for data-scare genomic tasks.\n",
      "Ultimately, the genome LLMs discussed in this review have applied ALM or MLM pretext tasks for pre-training, with\n",
      "minimal adaptations for biological context. The effectiveness of these models pre-training has been largely unexplored, though\n",
      "initial investigations on the performance of pre-trained models or on pre-training regimes for genome LLMs have not been\n",
      "favourable78,117. While this remains a limitation of current genome LLMs, it also provides a promising direction for future\n",
      "research.\n",
      "Future Directions\n",
      "The success of deep learning models for the genome, specifically with the increasing use of LLMs, and the limitations they are\n",
      "currently bounded by, provide a complex outlook for the future. Of the notable trends within deep learning genomic modelling,\n",
      "one of the most prominent is the potential of unsupervised pre-training regimes, and specifically of multi-species pre-training.\n",
      "This approach could capture evolutionarily conserved data in the genome and better model its underlying grammar. Furthermore,\n",
      "this approach transcends the need for specific architectures like the transformer, or the Hyena layer. However, with the success\n",
      "of many of these models taken as contingent upon their expensive and time-consuming pre-training regimes, it is important to\n",
      "20/33\n",
      "understand what exactly these models are learning in pre-training vs. fine-tuning. Recent work78investigating BERT model\n",
      "behaviour in genomics shows that k-mer embeddings from random data have comparable performance on downstream tasks to\n",
      "k-mer embeddings pre-trained on real biological sequences. Further work examining the zero-shot performance of models like\n",
      "scGPT and Geneformer suggests designing more biologically meaningful pretext tasks could greatly remedy their limitations117.\n",
      "This tells us that while pre-training and unsupervised learning have potential to increase the power of genomic models, the\n",
      "pre-training tasks for these models must be well designed and validated to prove true genomic grammar is being captured. We\n",
      "suggest further experiments are conducted on these models to compare pre-trained and fine-tuned vs. randomly initialized and\n",
      "fine-tuned embedding spaces. Additionally, while unsupervised pre-training provides an excellent direction for future research,\n",
      "we wonder if pre-training on the entire genome is the best way to leverage the power of pre-training. Repetitive non-coding\n",
      "sections of DNA make up nearly half of the human genome119, and could potentially overpower the ability of these models to\n",
      "learn more relevant signals from relatively less common but more important sequence regions.\n",
      "New LLM architectures like the Hyena layer are emerging, which don’t rely on attention mechanisms yet still support\n",
      "pre-training. These models may offer better scalability for genomic data compared to traditional transformers49,77,81. The\n",
      "attention mechanism’s quadratic complexity is a bottleneck for genomic modeling, especially when increasing the context\n",
      "window size is crucial. This opens the door for next-gen models like HyenaDNA to potentially outperform transformers49.\n",
      "While these newer models have their own drawbacks, mainly in interpretability, they pose a real challenge to transformers.\n",
      "If the transformer’s key advantage has been its scalable context window size, it will need significant improvements to\n",
      "stay competitive. Efforts are underway to refine attention mechanisms, such as introducing sliding windows120, enhancing\n",
      "efficiency47,121, and improving long-range interaction modeling in NLP48. Despite these advancements, transformers still lag\n",
      "behind in context window size when compared to models like the Hyena layer.\n",
      "GPT-4’s prevalence and power continues to raise more interest in creating LLMs for the genome, however, it is even more\n",
      "interesting to consider the field will move away from the LLM entirely. Large models that can integrate multi-modal data\n",
      "could unify genomic, transcriptomic, proteomic, and epigenomic data, offering a more holistic view of biological systems\n",
      "than LLMs can provide on their own. If trends in deep learning models for proteins are predictive of future trends in genomic\n",
      "sequence modelling, it is very likely the next model to try for the genome will be the Diffusion model122. If pre-training with\n",
      "evolutionarily varied data is a key to success in modelling genomic information, perhaps diffusion, which naturally models\n",
      "evolution, is the obvious choice. A recent paper by Alamdari et. al.123marries these concepts together for protein generation,\n",
      "showing evolutionary sequence data might be all we need, to great success.\n",
      "The future of powerful and interpretable deep learning models in genomics is one of personalized medicine, understanding\n",
      "evolutionary dynamics, drug discovery, synthetic biology, and more. We are at an extremely exciting time for the field, and\n",
      "we hope to see an increase in the use of multi-species pre-training, and more biologically motivated and downstream-task\n",
      "aligned approaches to designing pretext tasks. We believe research in this area will lead to the greatest success in modelling the\n",
      "genome, whether it be through transformer models, Hyena layers, or diffusion models. Additionally, it is our belief that deep\n",
      "learning models will only succeed in modelling genomic data if strides continue to be made toward their interpretation within\n",
      "genomic contexts107,124.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def extract_text_excluding_references(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF excluding the references section.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    full_text = \"\"\n",
    "    \n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            # Ensure text is properly encoded and cleaned\n",
    "            text = text.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\")\n",
    "            full_text += text + \"\\n\"\n",
    "    \n",
    "    # Remove the references section\n",
    "    references_index = full_text.lower().rfind(\"references\")\n",
    "    if references_index != -1:\n",
    "        full_text = full_text[:references_index]\n",
    "    \n",
    "    print(full_text)\n",
    "    return full_text\n",
    "\n",
    "\n",
    "def count_keywords(text, keywords):\n",
    "    \"\"\"\n",
    "    Count occurrences of keyword variations in the text.\n",
    "    \"\"\"\n",
    "    keyword_counts = {key[0]: 0 for key in keywords}  # Initialize counts for primary terms\n",
    "    keyword_sentences = {key[0]: [] for key in keywords}  # Store sentences with occurrences\n",
    "    \n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    for keyword_group in keywords:\n",
    "        main_keyword = keyword_group[0]\n",
    "        variations = keyword_group\n",
    "        for sentence in sentences:\n",
    "            for variation in variations:\n",
    "                if variation.lower() in sentence.lower():\n",
    "                    keyword_counts[main_keyword] += 1\n",
    "                    keyword_sentences[main_keyword].append(sentence.strip())\n",
    "    \n",
    "    return keyword_counts, keyword_sentences\n",
    "\n",
    "def process_papers(folder_path, keywords):\n",
    "    \"\"\"\n",
    "    Process all papers in a folder, count keywords, and compile results into a DataFrame.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    keyword_list = [k[0] for k in keywords]\n",
    "    sentence_results = {key[0]: [] for key in keywords}\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            text = extract_text_excluding_references(pdf_path)\n",
    "            keyword_counts, keyword_sentences = count_keywords(text, keywords)\n",
    "            results.append([filename] + [keyword_counts.get(keyword, 0) for keyword in keyword_list])\n",
    "            for key in keyword_sentences:\n",
    "                sentence_results[key].extend(keyword_sentences[key])\n",
    "    \n",
    "    columns = [\"Paper Name\"] + keyword_list\n",
    "    df = pd.DataFrame(results, columns=columns)\n",
    "    \n",
    "    return df, sentence_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define keyword groups\n",
    "    keywords = [\n",
    "        [\"Novelty\", \"novel\", \"innovative\", \"groundbreaking\"],\n",
    "        [\"Simplicity\", \"minimalistic\", \"concise\", \"parsimonious\", \"lightweight\"],\n",
    "        [\"Generalization\", \"generalisable\", \"generalizable\", \"transferability\", \"out-of-distribution\", \"domain adaptation\"],\n",
    "        [\"Flexibility/Extensibility\", \"flexible\", \"flexibility\", \"extensibility\", \"extensible\", \"adaptable\", \"modular\", \"scalable\"],\n",
    "        [\"Robustness\", \"resilient\", \"fault-tolerant\", \"noise tolerance\"], \n",
    "        [\"Realistic output\", \"authentic\", \"plausible\"], \n",
    "        [\"Formal description/analysis\", \"formal\", \"mathematical\", \"rigorous\", \"analytical\", \"axiomatic\", \"proof-based\"],\n",
    "        [\"Theoretical guarantees\", \"guarantee\", \"provable\", \"convergence proof\", \"theoretical bound\", \"performance bound\"],\n",
    "        [\"Approximation\", \"approximation theory\"], \n",
    "        [\"Quantitative evidence (e.g. experiments)\", \"quantitative\", \"numerical results\", \"empirical study\", \"measurable\"],\n",
    "        [\"Qualitative evidence (e.g. examples)\", \"case study\", \"illustrative\"], \n",
    "        [\"Scientific methodology\",  \"hypothesis-driven\", \"scientific\"], \n",
    "        [\"Controllability (of model owner)\", \"governability\", \"ownership\", \"model steering\"],\n",
    "        [\"Human-like mechanism\", \"biologically inspired\", \"cognitive\"],\n",
    "        [\"Low cost\", \"cheap\", \"cost\", \"affordable\", \"resource-efficient\", \"budget-friendly\"],\n",
    "        [\"Large scale\", \"scalability\", \"big data\", \"high-capacity\", \"massive-scale\"],\n",
    "        [\"Promising\"], \n",
    "        [\"Generality\", \"broad applicability\", \"domain-independent\", \"versatile\"],\n",
    "        [\"Principled\", \"theoretically sound\", \"axiomatic\", \"methodologically rigorous\"],\n",
    "        [\"Exactness\", \"error-free\"], \n",
    "        [\"Preciseness\", \"high-fidelity\"], \n",
    "        [\"Concreteness\", \"grounded\", \"verifiable\"], \n",
    "        [\"Automatic\", \"self-operating\", \"hands-free\"], \n",
    "        [\"Performance\"], \n",
    "        [\"Accuracy\", \"precision\", \"recall\", \"F1-score\", \"error rate\", \"reliability\"],\n",
    "        [\"Avoiding train/test discrepancy\", \"train/test\", \"discrepancy\", \"distribution shift\", \"generalization gap\"],\n",
    "        [\"State-of-the-art\", \"SOTA\", \"best performing\", \"cutting-edge\", \"latest\"],\n",
    "        [\"Efficiency\", \"efficient\"],\n",
    "        [\"Reduced training time\", \"training time\", \"fast training\", \"speed-up\", \"low latency\"],\n",
    "        [\"Memory efficiency\", \"memory-efficient\", \"low memory footprint\", \"RAM optimization\"],\n",
    "        [\"Data efficiency\", \"data-efficient\", \"few-shot\", \"self-supervised\", \"low data regime\"],\n",
    "        [\"Label efficiency (reduced need for labeled data)\", \"label-efficient\", \"semi-supervised\", \"weak supervision\"],\n",
    "        [\"Energy efficiency\", \"energy-efficient\", \"low power\", \"green AI\", \"sustainable AI\"],\n",
    "        [\"Effectiveness\"], \n",
    "        [\"Successful\"], \n",
    "        [\"Building on classic work\", \"classic work\", \"foundational\", \"historical perspective\"],\n",
    "        [\"Building on recent work\", \"recent work\", \"latest advancements\", \"current research\"],\n",
    "        [\"Unifying ideas or integrating components\", \"unifying\", \"integrative\", \"synergistic\", \"compositional\"],\n",
    "        [\"Identifying limitations\", \"limitations\", \"weaknesses\", \"failure modes\"], \n",
    "        [\"Critique\", \"criticism\", \"critical review\"],\n",
    "        [\"Understanding (for researchers)\", \"understanding\", \"conceptual clarity\"], \n",
    "        [\"Improvement\"],\n",
    "        [\"Progress\"],\n",
    "        [\"Used in practice/Popular\", \"used in practice\", \"popular\", \"adopted\", \"real-world usage\"],\n",
    "        [\"Reproducibility\", \"reproduce\", \"replication\", \"repeatability\", \"consistent results\"],\n",
    "        [\"Easy to implement\", \"simple to use\", \"straightforward\"],\n",
    "        [\"Requires few resources\", \"resources\", \"low-resource\", \"minimal requirements\"],\n",
    "        [\"Parallelizability / distributed\", \"parallelizability\", \"parallelization\", \"distributed\"],\n",
    "        [\"Facilitating use (e.g. sharing code)\", \"sharing code\", \"open-source\"], \n",
    "        [\"Scales up\", \"scale up\", \"expands\", \"large-scale deployment\"],\n",
    "        [\"Applies to real world\", \"real world\", \"practical application\", \"real-world relevance\"],\n",
    "        [\"Learning from humans\", \"human learning\", \"human-in-the-loop\", \"interactive learning\"],\n",
    "        [\"Practical\", \"applied AI\"], \n",
    "        [\"Useful\"], \n",
    "        [\"Interpretable (to users)\", \"interpretable\", \"explainable\"],\n",
    "        [\"Transparent (to users)\", \"transparent\", \"transparency\", \"accountability\"],\n",
    "        [\"Privacy\", \"privacy\", \"private\", \"confidentiality\", \"data protection\"],\n",
    "        [\"Fairness\", \"equitable\", \"bias mitigation\"], \n",
    "        [\"Not socially biased\", \"social bias\", \"socially biased\", \"fairness-aware\", \"bias-free\", \"equitable AI\"],\n",
    "        [\"User influence\", \"user impact\", \"user effect\", \"human influence\", \"user control\", \"user agency\"],\n",
    "        [\"Collective influence\", \"collective\", \"group influence\", \"crowd dynamics\", \"social influence\", \"peer effects\"],\n",
    "        [\"Deferral to humans\", \"human oversight\", \"human intervention\", \"human in the loop\", \"human-AI collaboration\"],\n",
    "        [\"Critiqability\", \"contestability\", \"scrutability\", \"reviewability\"], \n",
    "        [\"Beneficence\", \"beneficable\", \"welfare\", \"positive impact\", \"well-being\", \"prosocial\"],\n",
    "        [\"Non-maleficence\", \"harm avoidance\", \"ethical AI\", \"AI safety\", \"harm reduction\"],\n",
    "        [\"Justice\", \"equity\", \"bias mitigation\", \"equal treatment\", \"social justice\"],\n",
    "        [\"Respect for Persons\", \"human dignity\", \"respect for individuals\", \"respect for rights\", \"human rights\"],\n",
    "        [\"Autonomy (power to decide)\", \"autonomy\", \"autonome\", \"self-determination\", \"independence\", \"user agency\", \"free choice\"],\n",
    "        [\"Explicability\", \"explicable\", \"interpretability\", \"transparency\", \"explainability\", \"understandability\"],\n",
    "        [\"Respect for Law and public interest\", \"respect for law\", \"respect for public interest\", \"compliance\", \"regulatory adherence\", \"legal AI\", \"governance\"],\n",
    "        [\"Security\", \"secure\", \"cybersecurity\", \"privacy protection\", \"adversarial robustness\", \"data security\"],\n",
    "        [\"Easy to work with\", \"user-friendly\", \"ease of use\"],\n",
    "        [\"Realistic world model\", \"world model\", \"real-world applicability\", \"realistic simulation\", \"grounded AI\", \"embodied intelligence\"],\n",
    "        [\"Fast\", \"speed\", \"low latency\", \"real-time\"]\n",
    "    ]\n",
    "\n",
    "    # Process papers and save results\n",
    "    keyword_matrix, sentences = process_papers(\"Paper Test\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Name</th>\n",
       "      <th>Novelty</th>\n",
       "      <th>Simplicity</th>\n",
       "      <th>Generalization</th>\n",
       "      <th>Flexibility/Extensibility</th>\n",
       "      <th>Robustness</th>\n",
       "      <th>Realistic output</th>\n",
       "      <th>Formal description/analysis</th>\n",
       "      <th>Theoretical guarantees</th>\n",
       "      <th>Approximation</th>\n",
       "      <th>...</th>\n",
       "      <th>Non-maleficence</th>\n",
       "      <th>Justice</th>\n",
       "      <th>Respect for Persons</th>\n",
       "      <th>Autonomy (power to decide)</th>\n",
       "      <th>Explicability</th>\n",
       "      <th>Respect for Law and public interest</th>\n",
       "      <th>Security</th>\n",
       "      <th>Easy to work with</th>\n",
       "      <th>Realistic world model</th>\n",
       "      <th>Fast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLMs.pdf</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Paper Name  Novelty  Simplicity  Generalization  Flexibility/Extensibility  \\\n",
       "0   LLMs.pdf       14           1               3                          3   \n",
       "\n",
       "   Robustness  Realistic output  Formal description/analysis  \\\n",
       "0           0                 0                            0   \n",
       "\n",
       "   Theoretical guarantees  Approximation  ...  Non-maleficence  Justice  \\\n",
       "0                       0              1  ...                0        0   \n",
       "\n",
       "   Respect for Persons  Autonomy (power to decide)  Explicability  \\\n",
       "0                    0                           0             17   \n",
       "\n",
       "   Respect for Law and public interest  Security  Easy to work with  \\\n",
       "0                                    0         0                  0   \n",
       "\n",
       "   Realistic world model  Fast  \n",
       "0                      0     2  \n",
       "\n",
       "[1 rows x 75 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Novelty': ['At the same time, novel techniques for capturing genomic information9,10such as\\nchromatin accessibility11,12, methylation12,13, transcriptional status14,15, chromatin structure16and bound molecules12have\\nprovided a large and varied source of omics data to mine17.',\n",
       "  'Furthermore, transformers applied to genomic\\ndata offer a novel conceptual framework, the attention mechanism , to study the organization and grammar of the genome.',\n",
       "  'This will further accelerate the\\napplication of deep learning models for genomics, where methodologies for improving transformer efficiency are now being\\nadopted in newer models, and most recently, genomic models are being proposed with novel architectures that claim to be the\\n“next transformer”49.',\n",
       "  'The early research\\nin this space holds promising and innovative prospects for the field.',\n",
       "  'This is followed by a succinct review of previous deep learning architectures used in this field,\\nestablishing a foundation for an examination of novel architectures since.',\n",
       "  'The\\nattention mechanism is bounded by quadratic cost in sequence length, and while adaptations have been made to improve the\\nefficiency of the transformer47,48,79, novel architectures have been proposed to replace it entirely.',\n",
       "  'The novelty of this design also includes following the dual-encoders with a transformer module before the task-specific decoder.',\n",
       "  'The novelty of this design also includes following the dual-encoders with a transformer module before the task-specific decoder.',\n",
       "  'Along with scGPT’s novel tokenization scheme, the author’s introduced a novel attention mask for the generative pre-\\ntraining stage.',\n",
       "  'Limitations\\nAs previous review papers have focused on the limitations inherent in applying deep learning models to genomic data, including\\ncell-type specificity debates and training data limitations58, we focus on the limitations inherent to the application of novel\\narchitectures to genomics, such as the transformer and Hyena.',\n",
       "  'This is likely due to the novelty of the Hyena layer, as interpretability methods have not yet been explored for this architecture.',\n",
       "  'This is likely due to the novelty of the Hyena layer, as interpretability methods have not yet been explored for this architecture.',\n",
       "  'Furthermore, we encourage the concurrent development of interpretability methods for the\\nnovel architectures proposed to this field beyond transformers, like HyenaDNA49.',\n",
       "  'Similarly, we expect models that aim to unseat the transformer in\\ngenomics invest in interpretability methods along with novel architecture design.'],\n",
       " 'Simplicity': ['The glossary additionally provides concise definitions.'],\n",
       " 'Generalization': ['The model can then be fine-tuned for a specific task using a smaller labelled dataset, thereby benefiting from both the broad\\ngeneralization capabilities gained during pre-training and the task-specific adaptations acquired during fine-tuning73.',\n",
       "  'Hybrid models often forgo pre-training, a\\n12/33\\nhallmark feature commonly associated with LLMs. This lack of pre-training further distinguishes them from LLMs, which are\\ngenerally more versatile and generalizable.',\n",
       "  'The motivation for using a generative approach with “prompting”\\nto model single-cell omic data, beyond zero-shot generalization advantages, was twofold.'],\n",
       " 'Flexibility/Extensibility': ['Their applicability extends to genomic sequences, even being adaptable for tasks where the final goal of the model is not\\nto produce a sequence but output a quantitative assay or classification31,46.',\n",
       "  'The genomic sequence information is captured in convolutional layers by a series of operations with learned kernels that match\\nspecific DNA subsequences, demonstrating a versatile and adaptable method for processing genomic data55,63,84,85.',\n",
       "  'If the transformer’s key advantage has been its scalable context window size, it will need significant improvements to\\nstay competitive.'],\n",
       " 'Robustness': [],\n",
       " 'Realistic output': [],\n",
       " 'Formal description/analysis': [],\n",
       " 'Theoretical guarantees': [],\n",
       " 'Approximation': ['We can see the use of matrices of size k×n\\nfor each head shows that multi-head attention is a low-rank approximation for the full n×nattention matrix.'],\n",
       " 'Quantitative evidence (e.g. experiments)': ['Their applicability extends to genomic sequences, even being adaptable for tasks where the final goal of the model is not\\nto produce a sequence but output a quantitative assay or classification31,46.',\n",
       "  'Transformer-hybrid models make\\npredictions of experimental assays, either quantitative predictions like ChIP-seq intensities, or binary predictions like the\\npresence/absence of ChIP-seq peaks31,62.',\n",
       "  'Enformer represents an evolution in the prediction of quantitative genomic assays.',\n",
       "  'The ranking approach\\nmay even have benefits over reporting gene expression values quantitatively by offering a buffer against technical artefacts that\\nbias these counts.'],\n",
       " 'Qualitative evidence (e.g. examples)': [],\n",
       " 'Scientific methodology': ['For example, pre-training Gto estimate the evolutionary viability of a mutated sequence\\nwould be more aligned and scientifically sound.'],\n",
       " 'Controllability (of model owner)': [],\n",
       " 'Human-like mechanism': [],\n",
       " 'Low cost': ['Instead, in genomics, attention is often applied to features extracted by dilated convolutions before transformer modules,\\nthereby extending the range of self-attention calculations, at the cost of losing pairwise scores of relevance across a whole\\nsequence31,45,62.',\n",
       "  'This method can capture features that are more relevant to specific\\ntasks a model will later be fine-tuned on, reducing the computational cost of fine-tuning.',\n",
       "  'The\\nattention mechanism is bounded by quadratic cost in sequence length, and while adaptations have been made to improve the\\nefficiency of the transformer47,48,79, novel architectures have been proposed to replace it entirely.',\n",
       "  'These new model architectures\\nmust be able to be pre-trained like the transformer and exhibit an attention-like mechanism, without the cost of attention.',\n",
       "  'While this ensures the Hyena layer is parameter-efficient, it does not explain how the\\nlayer is sub-quadratic in cost.',\n",
       "  'Long convolutions cannot be naively applied, even implicit ones, as a convolution with a kernel\\nsize equal to the input length would result in a quadratic-cost computation like attention.',\n",
       "  'To remedy this, the Hyena authors\\nuse the Fast Fourier Transform82to bring the cost of computing a long convolution down, essentially moving the convolution\\ncalculation from the time domain (with timesteps t) to the frequency domain, or the Fourier domain.',\n",
       "  'This approach is often necessitated by the quadratic computational cost of attention\\nmechanisms.',\n",
       "  'While this approach to increasing context-window size was able to identify between TATA and non-TATA promoters\\nwell, they did not demonstrate an end-to-end approach in modeling complex long-range dependencies, due to the limitations of\\nthe cost of attention.',\n",
       "  'These modifications allow DNABERT-2 to perform comparably to the Nucleotide Transformer\\nmodels in several tasks, despite 21 times less parameters and significantly less computational cost.',\n",
       "  'This allows\\nthe model to capture dependencies across the entire input sequence without a prohibitive increase in computational cost or risk\\nof overfitting.',\n",
       "  'Compute Requirements\\nPerhaps the most noteworthy limitation in the usage of LLMs for the genome, irrespective of the use of transformer, convolution,\\nor Hyena layer, is the computational cost of training these models.',\n",
       "  'Their training regimes, pre-training in particular, often\\nnecessitate high-performance computing resources, which may not be readily accessible or affordable for all research teams.'],\n",
       " 'Large scale': ['The Hyena authors proposed that the global context and data-dependency properties of attention could be satisfied by\\nanother approach, one that would have better scalability in terms of context length.',\n",
       "  'These models may offer better scalability for genomic data compared to traditional transformers49,77,81.'],\n",
       " 'Promising': ['The early research\\nin this space holds promising and innovative prospects for the field.',\n",
       "  'While this remains a limitation of current genome LLMs, it also provides a promising direction for future\\nresearch.'],\n",
       " 'Generality': ['The genomic sequence information is captured in convolutional layers by a series of operations with learned kernels that match\\nspecific DNA subsequences, demonstrating a versatile and adaptable method for processing genomic data55,63,84,85.',\n",
       "  'Hybrid models often forgo pre-training, a\\n12/33\\nhallmark feature commonly associated with LLMs. This lack of pre-training further distinguishes them from LLMs, which are\\ngenerally more versatile and generalizable.'],\n",
       " 'Principled': [],\n",
       " 'Exactness': [],\n",
       " 'Preciseness': [],\n",
       " 'Concreteness': [],\n",
       " 'Automatic': [],\n",
       " 'Performance': ['This allows for available but unlabelled\\ndata to contribute to model performance, and improve the results the model would have on an otherwise limitingly small\\ndataset.',\n",
       "  'The training phase allows LLMs to learn the underlying structure of a dataset, and during the adaptation process\\n(fine-tuning or prompting, etc.) the downstream task performance is evaluated.',\n",
       "  'Normalization layers have long been used in deep learning as a\\nway to prevent over-fitting, improve generalisation performance and accelerate convergence (a state during training in which\\nthe model loss becomes stable)69.',\n",
       "  'Zero-shot\\ngeneralisation is a model’s performance on tasks the model was not explicitly trained on, and unsupervised pre-training is a\\nspecific training regime where models are trained on large amounts of unlabelled data through a pretext task.',\n",
       "  'Models exhibiting\\ngood zero-shot performance after pre-training on DNA sequences could capture multiple aspects of genomic grammar and\\nstructure instead of, for example, focusing on learning the best representation of a DNA sequence for predicting TATA-promoter\\nregions (which may be too-constrained a task for good generalisation beyond this task).',\n",
       "  'Supervised pre-training can sometimes\\nlead to faster convergence or better performance on the downstream target task.',\n",
       "  'They specialize in higher\\naccuracy performance on many of the same tasks as their CNN predecessors20,21.',\n",
       "  'DNABERT has state-of-the-art performance in predicting proximal and core promoter regions (two specific kinds of\\npromoter types), and presence of transcription factor binding sites.',\n",
       "  'The smallest of the Nucleotide Transformer models is five times larger than DNABERT, and the authors’ benchmarking\\nexperiments (predicting enhancers, promoters, TATA promoters, splice sites etc.) show that increasing model size yields better\\nperformance.',\n",
       "  'The Geneformer model reports excellent performance in gene dosage sensitivity predictions, chromatin dynamics predictions,\\nand network dynamics predictions.',\n",
       "  'In contrast, scGPT’s fine-tuning training regime employed a non-generative approach, instead using MLM for optimal\\n15/33\\nperformance.',\n",
       "  'scGPT, pre-trained on 33 million healthy human cells, achieves state-of-the-art performance in genetic perturbation\\nprediction, batch correction, and multi-omic integration, further demonstrating the power of “pre-training universally, fine-tuning\\non demand”.',\n",
       "  'While the authors state this may have improved performance, they do not show any\\nexperiments to validate this claim.',\n",
       "  'All in all, the model boasts state-of-the-art performance on all\\neight datasets from GenomicBenchmarks97.',\n",
       "  'Karollus et\\nal.showed that significantly reducing the input size given to the Enformer model has a minimal effect on its performance,\\nsuggesting that transformer-based models, like Enformer, may achieve state-of-the-art performance simply due to the addition\\nof the transformer module, or through increased parameter size.',\n",
       "  'However, there have not yet been experiments done to evaluate Borzoi’s62performance after a significant\\nreduction of the input size, similar to those performed on Enformer31.',\n",
       "  'Their training regimes, pre-training in particular, often\\nnecessitate high-performance computing resources, which may not be readily accessible or affordable for all research teams.',\n",
       "  'The effectiveness of these models pre-training has been largely unexplored, though\\ninitial investigations on the performance of pre-trained models or on pre-training regimes for genome LLMs have not been\\nfavourable78,117.',\n",
       "  'Recent work78investigating BERT model\\nbehaviour in genomics shows that k-mer embeddings from random data have comparable performance on downstream tasks to\\nk-mer embeddings pre-trained on real biological sequences.',\n",
       "  'Further work examining the zero-shot performance of models like\\nscGPT and Geneformer suggests designing more biologically meaningful pretext tasks could greatly remedy their limitations117.'],\n",
       " 'Accuracy': ['Skip-connections used between dilated residual convolutions, as in Akita, allow for a\\nmodel to learn to skip certain dilated convolutions across sequence if they do not contribute towards accurate prediction and\\ninstead hinder accuracy.',\n",
       "  'Encoder-only models, like BERT, are useful in\\nthe cases where final predictions have high accuracy when based off only an embedding, or feature representation of the\\ninputted sequence.',\n",
       "  'They specialize in higher\\naccuracy performance on many of the same tasks as their CNN predecessors20,21.',\n",
       "  'Long Range Interactions\\nTransformer-hybrid models, despite high accuracy on specific tasks for which large amounts of labelled training data exist,\\nstill appear unable to capture long-range dependencies within the genome.',\n",
       "  'While trends show models with increased context window size have had higher accuracy on predicting experimental assays,\\nthis does not necessarily suggest context window size is the driving force behind improved predictive capacity.'],\n",
       " 'Avoiding train/test discrepancy': [],\n",
       " 'State-of-the-art': ['DNABERT has state-of-the-art performance in predicting proximal and core promoter regions (two specific kinds of\\npromoter types), and presence of transcription factor binding sites.',\n",
       "  'scGPT, pre-trained on 33 million healthy human cells, achieves state-of-the-art performance in genetic perturbation\\nprediction, batch correction, and multi-omic integration, further demonstrating the power of “pre-training universally, fine-tuning\\non demand”.',\n",
       "  'All in all, the model boasts state-of-the-art performance on all\\neight datasets from GenomicBenchmarks97.',\n",
       "  'Karollus et\\nal.showed that significantly reducing the input size given to the Enformer model has a minimal effect on its performance,\\nsuggesting that transformer-based models, like Enformer, may achieve state-of-the-art performance simply due to the addition\\nof the transformer module, or through increased parameter size.'],\n",
       " 'Efficiency': ['Simultaneously, in the field of machine learning, computational advances continue to be made in improving the efficiency of the\\ntransformer, allowing the size of these models to increase along with their predictive power47,48.',\n",
       "  'This will further accelerate the\\napplication of deep learning models for genomics, where methodologies for improving transformer efficiency are now being\\nadopted in newer models, and most recently, genomic models are being proposed with novel architectures that claim to be the\\n“next transformer”49.',\n",
       "  'However, computational efficiency was becoming a bottleneck for RNNs due to hardware constraints which\\nprevented RNN-based models from expanding their parameter number44.',\n",
       "  'The transformers attention\\nallows them to account for relationships within sequences that are not determined by immediate proximity, like RNNs, while\\nleveraging parallelization on GPUs for computational efficiency, unlike RNNs66.',\n",
       "  'The attention function is then calculated in parallel (very efficiently\\non GPU) for each of these learned representations to produce dv-dimensional output values.',\n",
       "  'The\\nattention mechanism is bounded by quadratic cost in sequence length, and while adaptations have been made to improve the\\nefficiency of the transformer47,48,79, novel architectures have been proposed to replace it entirely.',\n",
       "  'Instead, an implicit convolution is parameter-efficient.',\n",
       "  'While this ensures the Hyena layer is parameter-efficient, it does not explain how the\\nlayer is sub-quadratic in cost.',\n",
       "  'Using convolutional\\nlayers, CNNs exploit spatial hierarchies and the concept of weight sharing, enabling efficient detection of regulatory patterns\\nand motifs regardless of their genomic location (within a certain kernel size) - a feature known as translational equivariance.',\n",
       "  'Furthermore, the BPE\\nmethod remains as computationally efficient as non-overlapping tokenization.',\n",
       "  'Along with the introduction of BPE instead of overlapping k-merization, the DNABERT-2 authors employ several other\\nmethods to improve computational efficiency over DNABERT, including the use of Flash Attention79, among others94,95.',\n",
       "  'Flash\\n14/33\\nAttention79is an IO efficient manipulation of the attention mechanism, optimizing for read and write time in memory as well\\nas improving computational efficiency by splitting the keys, queries and values into blocks that have softmax incrementally\\nperformed over the entire input.',\n",
       "  'Flash\\n14/33\\nAttention79is an IO efficient manipulation of the attention mechanism, optimizing for read and write time in memory as well\\nas improving computational efficiency by splitting the keys, queries and values into blocks that have softmax incrementally\\nperformed over the entire input.',\n",
       "  'In that case, transformer-based LLMs, without being preceded by downsampling techniques to reduce dimensionality prior\\nto the attention calculation, or applying a more efficiently implemented attention method79, will be limited by their context\\nwindow.',\n",
       "  'Efforts are underway to refine attention mechanisms, such as introducing sliding windows120, enhancing\\nefficiency47,121, and improving long-range interaction modeling in NLP48.'],\n",
       " 'Reduced training time': ['This alleviates the need\\nfor large amounts of labelled task-specific data, or extensive training time and compute spent when fine-tuning the model for\\nspecific downstream tasks.',\n",
       "  'The total amount of compute, in petaflop/s-days (PFS-Days) used to train the various models discussed in the\\nreview (all of the models for which parameter number, training time, and GPU usage was available).'],\n",
       " 'Memory efficiency': [],\n",
       " 'Data efficiency': ['The transformer model, along with some others, can address this imbalance by undertaking a pre-training\\nphase, where it can learn in an unsupervised orself-supervised manner from a large unlabelled dataset through a pretext\\ntask.',\n",
       "  'The major advantage of DNABERT lies in its capacity for self-supervised pre-training, a product of its transformer\\narchitecture.'],\n",
       " 'Label efficiency (reduced need for labeled data)': ['Pre-Training\\nPre-training can be unsupervised, supervised, or semi-supervised.',\n",
       "  'When the data has labels, the pre-training is\\nconsidered supervised, and when both labelled and unlabelled data is used, the pre-training is considered semi-supervised.',\n",
       "  'Semi-supervised pre-training combines the benefits of both supervised and unsupervised learning, and their drawbacks as\\nwell.'],\n",
       " 'Energy efficiency': [],\n",
       " 'Effectiveness': ['Pre-training Task Design\\nPre-training is a powerful and architecture-agnostic tool for genome LLMs, but its effectiveness hinges on the quality of the\\ntask assigned for pre-training.',\n",
       "  'The effectiveness of these models pre-training has been largely unexplored, though\\ninitial investigations on the performance of pre-trained models or on pre-training regimes for genome LLMs have not been\\nfavourable78,117.'],\n",
       " 'Successful': ['Introduction\\nIn the past decade, deep learning has gone from a niche technology to a tool that has been successfully applied to a diverse\\nrange of tasks, from generating art1,2, language representation3–6, and even to predicting protein structure from amino acid\\nsequence7.'],\n",
       " 'Building on classic work': ['Transformers\\nTo understand the use of transformers in the context of genomic modelling requires both a foundational understanding of the\\narchitecture and its usual training regime.',\n",
       "  'Another non-transformer genome LLM, HyenaDNA49, achieves a context size of 1 million nucleotides, 500x larger than\\nthe largest of the foundational models utilizing full pairwise attention, the Nucleotide Transformer46.'],\n",
       " 'Building on recent work': ['Recent work by Karollus et al.91scrutinized models like Enformer and Basenji286, emphasizing the need for more and\\nbetter-curated training data.',\n",
       "  'The idea behind this came from recent work that\\nshowed pre-trained CNNs are competitive with transformers in NLP74, and protein modelling96.',\n",
       "  'Recent work78investigating BERT model\\nbehaviour in genomics shows that k-mer embeddings from random data have comparable performance on downstream tasks to\\nk-mer embeddings pre-trained on real biological sequences.'],\n",
       " 'Unifying ideas or integrating components': [],\n",
       " 'Identifying limitations': ['Building on the foundation of traditional convolutional neural networks and recurrent\\nneural networks, we explore both the strengths and limitations of transformers and other LLMs for genomics.',\n",
       "  'The transformer’s\\nlack of recurrence was optimized for the attention mechanism to be computed in parallel on GPU hardware, overcoming the\\nRNNs computational limitations.',\n",
       "  'While each of these approaches to pre-training has their own strengths and limitations, unsupervised pre-training is an\\nimportant direction for future research.',\n",
       "  'They noted that despite Enformer’s dramatically increased context window, it still encountered\\nconsiderable limitations in predicting the impact of distal regulatory elements, such as enhancers.',\n",
       "  'While this approach to increasing context-window size was able to identify between TATA and non-TATA promoters\\nwell, they did not demonstrate an end-to-end approach in modeling complex long-range dependencies, due to the limitations of\\nthe cost of attention.',\n",
       "  'However, the non-overlapping k-mer approach comes with its own limitations, primarily that the insertion or deletion of a single\\nnucleotide base leads to dramatic changes in tokenized sequences39.',\n",
       "  'Limitations\\nAs previous review papers have focused on the limitations inherent in applying deep learning models to genomic data, including\\ncell-type specificity debates and training data limitations58, we focus on the limitations inherent to the application of novel\\narchitectures to genomics, such as the transformer and Hyena.',\n",
       "  'Calculations for PFS-Days can be found in\\nthe Limitations section.',\n",
       "  'Interpretability\\nBeyond limitations in capturing long-range dependencies, a fundamental limitation of applying deep learning models to\\ngenomic data lies in their black-box nature.',\n",
       "  'We acknowl-\\nedge that while previous papers provide some insight into transformers’ interpretability specifically in genomics105, they have\\nnot compared methods beyond attention scores or acknowledged the limitations of this method106.',\n",
       "  'While MLM\\naccounts for DNA’s bi-directional nature, it’s not without its own limitations.',\n",
       "  'Future Directions\\nThe success of deep learning models for the genome, specifically with the increasing use of LLMs, and the limitations they are\\ncurrently bounded by, provide a complex outlook for the future.',\n",
       "  'Further work examining the zero-shot performance of models like\\nscGPT and Geneformer suggests designing more biologically meaningful pretext tasks could greatly remedy their limitations117.'],\n",
       " 'Critique': [],\n",
       " 'Understanding (for researchers)': ['Although numerous review papers have explored deep\\nlearning models in genomics, with topics spanning from general introductions to specific discussions on model interpretation,\\nunderstanding gene regulation, predicting the impact of genetic variation, and unveiling new applications18,50–60, we take a\\ndifferent approach.',\n",
       "  'This review is\\nintended for computational biologists with deep learning experience interested in understanding LLMs for the genome, as well\\nas for computer scientists who are keen on gaining insights into the research opportunities within this exciting field.',\n",
       "  'These tasks are\\nnot the final tasks the model will be evaluated on, but instead work to help the model gain an understanding of the\\norganization and grammar of the genome (in the context of genomics).',\n",
       "  'Transformers\\nTo understand the use of transformers in the context of genomic modelling requires both a foundational understanding of the\\narchitecture and its usual training regime.',\n",
       "  'BERT-based models pre-trained with MLM are useful for\\nunderstanding genomic sequences where the overall context (both upstream and downstream) is important, such as identifying\\ngenomic features or classifying sequences where directionality is not critical.',\n",
       "  'The objective of this technique is to enable the model to understand the contexts\\nin which each base can occur, thus gaining a holistic understanding of the genomic sequence.',\n",
       "  'As such, ALM and MLM provide different types of\\nsequence understanding and can be useful for different types of tasks in genomics.',\n",
       "  'However, its unidirectional nature means it might be less effective at understanding\\nthe broader context of a sequence than bidirectional models based on BERT3,30, which consider both previous and subsequent\\ntokens in the sequence.',\n",
       "  'Therefore, we split the transformer-based models for genomics into two classes: hybrids and LLMs. Hybrid models, which\\nincorporate transformers as one element within a more intricate architecture, may not meet the criteria for being considered true\\nLLMs. Unlike LLMs, which are designed for comprehensive understanding and generation of language-like sequences, hybrid\\nmodels are specialized for tasks such as predicting experimental assays like Cap Analysis Gene Expression (CAGE) tracks and\\nChIP-seq.',\n",
       "  'The future of powerful and interpretable deep learning models in genomics is one of personalized medicine, understanding\\nevolutionary dynamics, drug discovery, synthetic biology, and more.'],\n",
       " 'Improvement': ['Most improvements in assay prediction models\\nare aimed at increasing context window in an effort to better model these long-range dependencies.',\n",
       "  'If the transformer’s key advantage has been its scalable context window size, it will need significant improvements to\\nstay competitive.'],\n",
       " 'Progress': ['Readers interested in the applications as well as progress and challenges of these tools should read Routhier et al.59\\nand Spoval et al.60.'],\n",
       " 'Used in practice/Popular': ['However, as the popularity of transformer models44in the fields of computer vision and natural language processing (NLP),\\namong others, has increased, so too has their application to genomic modelling problems30,31,39–41,45,46.',\n",
       "  'This will further accelerate the\\napplication of deep learning models for genomics, where methodologies for improving transformer efficiency are now being\\nadopted in newer models, and most recently, genomic models are being proposed with novel architectures that claim to be the\\n“next transformer”49.',\n",
       "  'However, decoder-only models have\\nyet to be widely adopted for transformers in genomics.',\n",
       "  'While the RNN enjoyed a lot of success in the NLP field, it was not broadly adopted for genomic modelling24,29,43,89.',\n",
       "  'DNABERT’s k-merization scheme for tokenizing the genome was adopted by many transformer-based LLMs for the\\ngenome, including the Nucleotide Transformer46.',\n",
       "  'Some transformer\\nmodels have recently been proposed to mitigate the bias of cross-cell lines gene expression prediction using transfer learning,\\nbut this approach has yet to be commonly adopted102.'],\n",
       " 'Reproducibility': [],\n",
       " 'Easy to implement': [],\n",
       " 'Requires few resources': ['Their training regimes, pre-training in particular, often\\nnecessitate high-performance computing resources, which may not be readily accessible or affordable for all research teams.'],\n",
       " 'Parallelizability / distributed': ['The transformers attention\\nallows them to account for relationships within sequences that are not determined by immediate proximity, like RNNs, while\\nleveraging parallelization on GPUs for computational efficiency, unlike RNNs66.'],\n",
       " 'Facilitating use (e.g. sharing code)': [],\n",
       " 'Scales up': ['Similarly,\\nthe Hyena75layer’s ability to take long convolutions over whole sequences with data-controlled gating expands sequence\\ncontext while reducing the dimensionality of the sequence information.',\n",
       "  'Borzoi further expands the number\\nof experimental assay predictions compared to Enformer, and introduces predictions of RNA-seq coverage.',\n",
       "  'As of now, it\\nremains unclear whether the success of the transformer model lies in an artefact of the architecture, like the attention mechanism,\\nor whether this mechanism simply allowed these models to scale up more quickly than their counterparts.'],\n",
       " 'Applies to real world': [],\n",
       " 'Learning from humans': [],\n",
       " 'Practical': [],\n",
       " 'Useful': ['Theis10, 11, 12, 13, Alan Moses1, 14,\\nand Bo Wang1, 2, 3, 15*\\n1Department of Computer Science, University of Toronto, Toronto, Ontario, Canada\\n2Vector Institute for Artificial Intelligence, Toronto, Ontario, Canada\\n3Peter Munk Cardiac Center, University Health Network, Toronto, Ontario, Canada\\n4Prosserman Centre for Population Health Research, Lunenfeld-Tanenbaum Research Institute, Toronto, Ontario,\\nCanada\\n5Department of Molecular Genetics, University of Toronto, Toronto, Ontario, Canada\\n6The Donnelly Centre, University of Toronto, Toronto, Ontario, Canada\\n7Department of Biochemistry & Biophysics, University of California, San Francisco, San Francisco, California, USA\\n8Department of Urology, University of California, San Francisco, San Francisco, California, USA\\n9Helen Diller Family Comprehensive Cancer Center, University of California, San Francisco, San Francisco,\\nCalifornia, USA\\n10Institute of Computational Biology, Department of Computational Health, Helmholtz Munich, Munich, Germany\\n11TUM School of Life Sciences Weihenstephan, Technical University of Munich, Munich, Germany\\n12Department of Mathematics, School of Computation, Information and Technology, Technical University of Munich,\\nGarching, Germany\\n13Munich Center for Machine Learning, Technical University of Munich, Garching, Germany\\n14Department of Cell and System Biology, University of Toronto, Toronto, Ontario, Canada\\n15Department of Laboratory Medicine and Pathobiology, University of Toronto, Toronto, Ontario, Canada\\n*e-mail: bowang@vectorinstitute.ai\\nABSTRACT\\nIn the rapidly evolving landscape of genomics, deep learning has emerged as a useful tool for tackling complex computational\\nchallenges.',\n",
       "  'The encoder-decoder framework proved particularly useful in the Orca71paper, as using the encoder-decoder structure does not\\nconstrain input and output sequences to be the same length, accommodating the complexity and variability inherent in genomic\\ndata.',\n",
       "  'Encoder-only models, like BERT, are useful in\\nthe cases where final predictions have high accuracy when based off only an embedding, or feature representation of the\\ninputted sequence.',\n",
       "  'BERT-based models pre-trained with MLM are useful for\\nunderstanding genomic sequences where the overall context (both upstream and downstream) is important, such as identifying\\ngenomic features or classifying sequences where directionality is not critical.',\n",
       "  'As such, ALM and MLM provide different types of\\nsequence understanding and can be useful for different types of tasks in genomics.',\n",
       "  'WeightedSHAP extends this by considering the\\nimportance of each data point, useful when some instances matter more than others.',\n",
       "  'To maximize the benefits of pre-training for a genomic LLM, the initial task\\nshould be both biologically relevant and useful for later applications.'],\n",
       " 'Interpretable (to users)': ['Additionally, the\\nmodel was proposed as “interpretable” due to direct analysis of the attention matrix.',\n",
       "  'However, treating the attention mechanism\\nwithin transformers as directly interpretable has drawbacks90.',\n",
       "  'However, several studies on\\ntransformer models outside the context of genomics have shown attention is not inherently interpretable106,107.',\n",
       "  'The transformer-based models covered in this review have reported attention scores for their interpretability metrics, with\\nthe SATORI115authors even claiming the model was inherently interpretable due to the attention mechanism alone.',\n",
       "  'The future of powerful and interpretable deep learning models in genomics is one of personalized medicine, understanding\\nevolutionary dynamics, drug discovery, synthetic biology, and more.'],\n",
       " 'Transparent (to users)': [],\n",
       " 'Privacy': ['Data Privacy\\nAs mentioned, the majority of the models discussed in this review are trained on public datasets like ENCODE98and Roadmap99.',\n",
       "  'Data Privacy\\nAs mentioned, the majority of the models discussed in this review are trained on public datasets like ENCODE98and Roadmap99.',\n",
       "  'As more data continues to be integrated into these models, and these models are potentially leveraged for\\nuse on private datasets including in clinical settings, there is an increasing need for developing and implementing more robust\\nde-identification techniques and privacy-preserving algorithms, such as differential privacy103and federated learning104.',\n",
       "  'As more data continues to be integrated into these models, and these models are potentially leveraged for\\nuse on private datasets including in clinical settings, there is an increasing need for developing and implementing more robust\\nde-identification techniques and privacy-preserving algorithms, such as differential privacy103and federated learning104.',\n",
       "  'As more data continues to be integrated into these models, and these models are potentially leveraged for\\nuse on private datasets including in clinical settings, there is an increasing need for developing and implementing more robust\\nde-identification techniques and privacy-preserving algorithms, such as differential privacy103and federated learning104.'],\n",
       " 'Fairness': [],\n",
       " 'Not socially biased': [],\n",
       " 'User influence': [],\n",
       " 'Collective influence': ['These nearby neurons share the weight, or connection strength, which is collectively updated\\nduring the backpropagation process.'],\n",
       " 'Deferral to humans': [],\n",
       " 'Critiqability': [],\n",
       " 'Beneficence': [],\n",
       " 'Non-maleficence': [],\n",
       " 'Justice': [],\n",
       " 'Respect for Persons': [],\n",
       " 'Autonomy (power to decide)': [],\n",
       " 'Explicability': ['The interpretability column is coloured by presence or absence of model-interpretability analysis.',\n",
       "  'Interpretability\\nBeyond limitations in capturing long-range dependencies, a fundamental limitation of applying deep learning models to\\ngenomic data lies in their black-box nature.',\n",
       "  'Previous papers have explored the interpretability of deep learning models in the context of\\ngenomics extensively18,55,58, here we focus specifically on the task of interpreting transformer models and similar architectures.',\n",
       "  'Attention scores of transformers have been proposed as an interpretability solution to the problem of black box models\\nin genomics30,31,105.',\n",
       "  'Beyond adapting interpretability methods for transformers, model-agnostic methods like SHAP (SHapley Additive\\nexPlanations)113, and more recently, weightedSHAP114, provide obvious avenues of exploration.',\n",
       "  'We acknowl-\\nedge that while previous papers provide some insight into transformers’ interpretability specifically in genomics105, they have\\nnot compared methods beyond attention scores or acknowledged the limitations of this method106.',\n",
       "  'Outside of the transformer models covered in this review, the other LLM models must be evaluated for their interpretability\\nin the context of genomic tasks.',\n",
       "  'GPN77reported motifs captured in the convolutional blocks of the architecture as a metric\\nof interpretability, similar to previous CNN-based models for genomics20,21,23.',\n",
       "  'Interestingly, despite\\nHyenaDNA’s49success in genomic modelling, the authors did not dedicate a section of their paper to model interpretability.',\n",
       "  'This is likely due to the novelty of the Hyena layer, as interpretability methods have not yet been explored for this architecture.',\n",
       "  'The transformer-based models covered in this review have reported attention scores for their interpretability metrics, with\\nthe SATORI115authors even claiming the model was inherently interpretable due to the attention mechanism alone.',\n",
       "  'While these\\nmodels have not employed more sophisticated methods for model interpretability, attention mechanisms appear to encourage\\nexploration of model interpretability.',\n",
       "  'In contrast, only one of the non-transformer LLM models (GPN77) reported some kind of\\nmodel interpretability.',\n",
       "  'Furthermore, we encourage the concurrent development of interpretability methods for the\\nnovel architectures proposed to this field beyond transformers, like HyenaDNA49.',\n",
       "  'We expect the use of interpretability methods for attention-based mechanisms in genomics to increase, given their increasing\\nusage trend among transformers applied to other fields107.',\n",
       "  'Similarly, we expect models that aim to unseat the transformer in\\ngenomics invest in interpretability methods along with novel architecture design.',\n",
       "  'While these newer models have their own drawbacks, mainly in interpretability, they pose a real challenge to transformers.'],\n",
       " 'Respect for Law and public interest': [],\n",
       " 'Security': [],\n",
       " 'Easy to work with': [],\n",
       " 'Realistic world model': [],\n",
       " 'Fast': ['Supervised pre-training can sometimes\\nlead to faster convergence or better performance on the downstream target task.',\n",
       "  'To remedy this, the Hyena authors\\nuse the Fast Fourier Transform82to bring the cost of computing a long convolution down, essentially moving the convolution\\ncalculation from the time domain (with timesteps t) to the frequency domain, or the Fourier domain.']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
